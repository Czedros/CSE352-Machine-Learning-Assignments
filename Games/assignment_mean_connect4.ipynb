{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Czedros/CSE352-Machine-Learning-Assignments/blob/main/Games/assignment_mean_connect4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyISr3c6L8cY"
      },
      "source": [
        "# Adversarial Search: Playing \"Mean\" Connect 4\n",
        "\n",
        "\n",
        "## Instructions\n",
        "\n",
        "Name: Kay Zhang\n",
        "\n",
        "I understand that my submission needs to be my own work: [enter 'yes']\n",
        "\n",
        "Points: 10\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Complete this notebook and submit it (save/print as pdf). The notebook needs to be a complete project report with your implementation, documentation including a short discussion of how your implementation works and your design choices, and experimental results (e.g., tables and charts with simulation results) with a short discussion of what they mean. Use the provided notebook cells and insert additional code and markdown cells as needed.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "You will implement different versions of agents that play \"Mean\" Connect 4:\n",
        "\n",
        "> \"Connect 4 is a two-player connection board game, in which the players choose a color and then take turns dropping colored discs into a seven-column, six-row vertically suspended grid. The pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one's own discs.\" (see [Connect Four on Wikipedia](https://en.wikipedia.org/wiki/Connect_Four))\n",
        "\n",
        "> **The mean part:** This game has an additional rule. Every time it is a player's turn, the player can decide to instead of playing a new disk, take a bottom row disk of the opponent and place it back in the top of the same column. All disks above the removed disk will fall down one position and the removed one will be placed on top. Note that a player can only move an _opponent's disc_ that is in the _bottom row_ of the board. **Further, you are not allowed to play a mean move if your opponent just played one.** This ensures the game will end at some point. This also may affect the definition of a state, compared with standard Connect 4.\n",
        "\n",
        "If a mean move causes both players to win, the game immediately ends and it is a tie, even if one player has more connect-4s than the other one. If a mean move causes one player to win, then the game also ends and the player with the connect-4 is the winner.\n",
        "\n",
        "Note that normal [Connect-4 has been solved](https://en.wikipedia.org/wiki/Connect_Four#Mathematical_solution)\n",
        "in 1988. A connect-4 solver with a discussion of how to solve different parts of the problem can be found here: https://connect4.gamesolver.org/en/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuWbcK6gL8cZ"
      },
      "source": [
        "## Task 1: Defining the Search Problem [1 point]\n",
        "\n",
        "Define the components of the search problem associated with this game:\n",
        "\n",
        "* Initial state\n",
        "* Actions\n",
        "* Transition model\n",
        "* Test for the terminal state\n",
        "* Utility for terminal states"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 Solution\n",
        "##Initial State\n",
        "The board is 7 columns by 6 rows, all cells are initially empty.\n",
        "\n",
        "Player 1 moves first.\n",
        "\n",
        "A flag last_move_was_mean set to False (since no move has been made).\n",
        "\n",
        "##Actions\n",
        "\n",
        "Regular Move: Drop a disc in any of the 7 columns that is not already full.\n",
        "   - The disc occupies the lowest empty cell in that column.\n",
        "\n",
        "Mean Move: If the opponent has a disc in the bottom cell of any column and the previous move was not mean, the current player can:\n",
        "   - Take the opponent's disc from the bottom cell of that column.\n",
        "   - \"Lift\" it to the topmost empty cell of the same column.\n",
        "   - The discs that were in between fall down by one cell.\n",
        "   - Mark the \"mean move\" flag as True, which disallows the next player from making a mean move immediately afterward.\n",
        "\n",
        "## 1.3 Transition Model\n",
        "- For a **Regular Move**:\n",
        "  - The new state is obtained by adding the current player's disc to the lowest empty row in the chosen column.\n",
        "  - The \"mean move\" flag is set to False\n",
        "\n",
        "- For a **Mean Move**:\n",
        "  - Remove the opponent's disc from the bottom cell of the chosen column.\n",
        "  - Shift all discs above it one row down.\n",
        "  - Place the removed disc in the highest empty cell of that column\n",
        "  - Set the \"mean move\" flag to True.\n",
        "\n",
        "## 1.4 Test for Terminal State\n",
        "Terminal State Test:\n",
        "\n",
        "A player has four connected disks (horizontally, vertically, or diagonally).\n",
        "\n",
        "The board is full (no legal moves left).\n",
        "\n",
        "A mean move causes both players to win (tie).\n",
        "\n",
        "## 1.5 Utility for Terminal States\n",
        "- +1 for the maximizing (current) player if they have a winning connect-4.\n",
        "- -1 for the minimizing (opponent) player if the current player loses.\n",
        "-  0 if it is a tie."
      ],
      "metadata": {
        "id": "pB_g218IGxtH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHDORt8_L8cg"
      },
      "source": [
        "#How big is the state space? Give an estimate and explain it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A very rough upper bound for standard Connect-4 is often estimated as follows:\n",
        "- Each cell can be in 3 possible states: empty, Player 1 disc, Player 2 disc.\n",
        "- With 42 cells, that yields 3^42 possible board configurations.\n",
        "\n",
        "**However**, in Mean Connect 4, we also track:\n",
        "1. Which player's turn it is (factor of 2).\n",
        "2. Whether the last move was mean or not (another factor of 2).\n",
        "\n",
        "So an upper bound could be on the order of:\n",
        "2 x 2 x 3^42.\n",
        "\n",
        "Which is approximately 1.41 x 10^21.\n"
      ],
      "metadata": {
        "id": "RfIrax0zsbOm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ci9AS2QL8ch"
      },
      "source": [
        "#How big is the game tree that minimax search will go through? Give an estimate and explain it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a branching factor of 14, given that there are 7 possible regular and 7 possible \"mean\" moves."
      ],
      "metadata": {
        "id": "_iHIPGpmBc8f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u66LNfUEL8ci"
      },
      "source": [
        "## Task 2: Game Environment and Random Agent [3 point]\n",
        "\n",
        "You can use a numpy character array as the board. Note that the following function can create boards of different sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gFoRCZefL8cj",
        "outputId": "875e5af6-391c-4ab6-fee1-cfd60c7fd80f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def empty_board(shape=(6, 7)):\n",
        "    return np.full(shape=shape, fill_value=0)\n",
        "\n",
        "print(empty_board())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCCwVya8L8cj"
      },
      "source": [
        "Instead of colors (red and yellow), you can use 1 and -1 to represent the players Max and Min. Make sure that your agent functions all have the from: `agent_type(state, player = 1)`, where board is the current board position and player is the player (1, -1) whose next move it is and who the agent should play."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtsGKQPsL8ck"
      },
      "source": [
        "Visualization code by Randolph Rankin:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dRChZkdZL8ck",
        "outputId": "93cf152b-5e40-4a66-b68f-a3ba69d0a94e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize(board):\n",
        "    plt.axes()\n",
        "    rectangle=plt.Rectangle((-0.5,len(board)*-1+0.5),len(board[0]),len(board),fc='blue')\n",
        "    circles=[]\n",
        "    for i,row in enumerate(board):\n",
        "        for j,val in enumerate(row):\n",
        "            color='white' if val==0 else 'red' if val==1 else 'yellow'\n",
        "            circles.append(plt.Circle((j,i*-1),0.4,fc=color))\n",
        "\n",
        "    plt.gca().add_patch(rectangle)\n",
        "    for circle in circles:\n",
        "        plt.gca().add_patch(circle)\n",
        "\n",
        "    plt.axis('scaled')\n",
        "    plt.show()\n",
        "\n",
        "board = [[0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 1, 0, 0, 0],\n",
        "         [0, 0, 0, 1, 0, 0, 0],\n",
        "         [0,-1,-1, 1,-1, 0, 0]]\n",
        "\n",
        "visualize(board)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOa_fihRL8ck"
      },
      "source": [
        "Implement helper functions for:\n",
        "\n",
        "* The transition model $result(s, a)$.\n",
        "* The utility function $utility(s)$.\n",
        "* Check for terminal states $terminal(s)$.\n",
        "* A check for available actions in each state $actions(s)$.\n",
        "\n",
        "Make sure that all these functions work with boards of different sizes (number of columns and rows)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8sDMBWFtL8ck"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def _column_is_full(board, col):\n",
        "    \"\"\"Check if a given column col is full (i.e. top cell is nonzero).\"\"\"\n",
        "    return board[0, col] != 0\n",
        "\n",
        "def _lowest_empty_row(board, col):\n",
        "    \"\"\"\n",
        "    Return the index of the lowest empty row in column 'col',\n",
        "    or None if the column is full.\n",
        "    \"\"\"\n",
        "    rows = board.shape[0]\n",
        "    for row in range(rows-1, -1, -1):\n",
        "        if board[row, col] == 0:\n",
        "            return row\n",
        "    return None  # if no empty cell\n",
        "\n",
        "def actions(board, player, last_move_was_mean=False):\n",
        "    \"\"\"\n",
        "    Return all possible actions for 'player' in the current board state.\n",
        "    Each action is a tuple: (\"regular\", col) or (\"mean\", col).\n",
        "    \"\"\"\n",
        "    rows, cols = board.shape\n",
        "    opponent = -player\n",
        "    valid_actions = []\n",
        "\n",
        "    # --- 1) Regular moves ---\n",
        "    for col in range(cols):\n",
        "        if not _column_is_full(board, col):\n",
        "            valid_actions.append((\"regular\", col))\n",
        "\n",
        "    # --- 2) Mean moves ---\n",
        "    if not last_move_was_mean:\n",
        "        bottom_row = rows - 1\n",
        "        for col in range(cols):\n",
        "            if board[bottom_row, col] == opponent:\n",
        "                if board[0, col] == 0:  # top is empty => we can move that bottom disc up top\n",
        "                    valid_actions.append((\"mean\", col))\n",
        "\n",
        "    return valid_actions\n",
        "\n",
        "def result(board, action, player, last_move_was_mean=False):\n",
        "    \"\"\"\n",
        "    Apply the given action (\"regular\" or \"mean\") in a *copy* of 'board' for 'player'.\n",
        "    Returns (new_board, next_player, new_last_mean_flag).\n",
        "    \"\"\"\n",
        "    rows, cols = board.shape\n",
        "    new_board = board.copy()\n",
        "    action_type, col = action\n",
        "    opponent = -player\n",
        "\n",
        "    if action_type == \"regular\":\n",
        "        # Place player's disc in the lowest empty position of col.\n",
        "        row_to_fill = _lowest_empty_row(new_board, col)\n",
        "        if row_to_fill is None:\n",
        "            raise ValueError(f\"Column {col} is full; cannot do a regular move here.\")\n",
        "        new_board[row_to_fill, col] = player\n",
        "\n",
        "        return new_board, opponent, False  # Next player is the opponent; last move was NOT mean\n",
        "\n",
        "    elif action_type == \"mean\":\n",
        "        # We remove opponent's disc from bottom cell,\n",
        "        # shift everything above it down by one,\n",
        "        # then place that disc in the topmost empty cell in the same col.\n",
        "        bottom_row = rows - 1\n",
        "\n",
        "        if new_board[bottom_row, col] != opponent:\n",
        "            raise ValueError(\"Invalid mean move: bottom cell is not the opponent's disc.\")\n",
        "        new_board[bottom_row, col] = 0\n",
        "\n",
        "        for r in range(bottom_row - 1, -1, -1):\n",
        "            new_board[r+1, col] = new_board[r, col]\n",
        "        new_board[0, col] = 0\n",
        "\n",
        "        row_to_fill = _lowest_empty_row(new_board, col)\n",
        "        if row_to_fill is None:\n",
        "            raise ValueError(\"Invalid mean move: no space at the top? Shouldn't happen if action is valid.\")\n",
        "        new_board[row_to_fill, col] = opponent\n",
        "\n",
        "        return new_board, opponent, True  # Next player is the opponent; last move WAS mean\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown action type: {action_type}\")\n",
        "\n",
        "def terminal(board):\n",
        "    \"\"\"\n",
        "    Check if the position is terminal (game over).\n",
        "    Returns True if terminal, else False.\n",
        "    \"\"\"\n",
        "    # 1) Check if either/both players has a connect-4\n",
        "    p1_win = has_connect4(board, +1)\n",
        "    p2_win = has_connect4(board, -1)\n",
        "\n",
        "    if p1_win or p2_win:\n",
        "        # If both are True (only possible via a mean move), the game ends in a tie\n",
        "        return True\n",
        "\n",
        "    # If the top row has no empty cells, the board is effectively full\n",
        "    if np.all(board[0] != 0):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def utility(board):\n",
        "    \"\"\"\n",
        "    Returns a utility value for the board:\n",
        "      +1 if Player +1 (Max) has connect-4\n",
        "      -1 if Player -1 (Min) has connect-4\n",
        "       0 if neither or both (tie).\n",
        "    \"\"\"\n",
        "    p1_win = has_connect4(board, +1)\n",
        "    p2_win = has_connect4(board, -1)\n",
        "\n",
        "    if p1_win and not p2_win:\n",
        "        return +1\n",
        "    elif p2_win and not p1_win:\n",
        "        return -1\n",
        "    else:\n",
        "        # Either tie or no connect-4\n",
        "        return 0\n",
        "\n",
        "def has_connect4(board, player):\n",
        "    \"\"\"\n",
        "    Check if 'player' (+1 or -1) has a 4-in-a-row anywhere on the board.\n",
        "    This works for boards of arbitrary shape.\n",
        "    Helper for a helper... Yes.\n",
        "    \"\"\"\n",
        "    rows, cols = board.shape\n",
        "\n",
        "    for r in range(rows):\n",
        "        for c in range(cols - 3):\n",
        "            window = board[r, c:c+4]\n",
        "            if np.all(window == player):\n",
        "                return True\n",
        "\n",
        "    for r in range(rows - 3):\n",
        "        for c in range(cols):\n",
        "            window = board[r:r+4, c]\n",
        "            if np.all(window == player):\n",
        "                return True\n",
        "\n",
        "    for r in range(rows - 3):\n",
        "        for c in range(cols - 3):\n",
        "            diag = [board[r + i, c + i] for i in range(4)]\n",
        "            if np.all(np.array(diag) == player):\n",
        "                return True\n",
        "\n",
        "    for r in range(rows - 3):\n",
        "        for c in range(3, cols):\n",
        "            diag = [board[r + i, c - i] for i in range(4)]\n",
        "            if np.all(np.array(diag) == player):\n",
        "                return True\n",
        "\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T7SYQM4L8cl"
      },
      "source": [
        "Implement an agent that plays randomly. Make sure the agent function receives as the percept the game state and returns a valid action. Use an agent function definition with the following signature (arguments):\n",
        "\n",
        "`def random_player(state, player = None): ...`\n",
        "\n",
        "The argument `player` is used for agents that do not store what side they are playing. The value passed on by the environment should be 1 ot -1 for playerred and yellow, respectively.  See [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "g0uUKNKNL8cl"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def random_player(board, player, last_move_was_mean=False):\n",
        "    legal_moves = actions(board, player, last_move_was_mean)\n",
        "    return random.choice(legal_moves) if legal_moves else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS3yOFlbL8cl"
      },
      "source": [
        "Let two random agents play against each other 1000 times. Look at the [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) to see how the environment uses the agent functions to play against each other.\n",
        "\n",
        "How often does each player win? Is the result expected?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "tags": [],
        "id": "55tvNH3vL8cl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0596e6f-5983-4728-fd30-5065fe66e859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Over 1000 random vs. random games:\n",
            "Player 1 wins: 583\n",
            "Player 2 wins: 395\n",
            "Ties:          22\n"
          ]
        }
      ],
      "source": [
        "def play_game(agent1, agent2, verbose=False):\n",
        "    \"\"\"\n",
        "    Let agent1 (player=+1) and agent2 (player=-1) play until terminal.\n",
        "    Return the utility of the final board: +1, -1, or 0 (tie).\n",
        "    \"\"\"\n",
        "    board = empty_board((6,7))\n",
        "    current_player = +1   # agent1 goes first\n",
        "    last_move_was_mean = False\n",
        "\n",
        "    while True:\n",
        "        if terminal(board):\n",
        "            break\n",
        "\n",
        "        if current_player == +1:\n",
        "            action = agent1(board, +1, last_move_was_mean)\n",
        "        else:\n",
        "            action = agent2(board, -1, last_move_was_mean)\n",
        "\n",
        "        # If no legal moves, we are effectively stuck => terminal\n",
        "        if not action:\n",
        "            break\n",
        "\n",
        "        new_board, next_player, new_mean_flag = result(board, action, current_player, last_move_was_mean)\n",
        "\n",
        "        # Update\n",
        "        board = new_board\n",
        "        current_player = next_player\n",
        "        last_move_was_mean = new_mean_flag\n",
        "\n",
        "        if verbose:\n",
        "            print(board, \"\\n\")\n",
        "\n",
        "    return utility(board)\n",
        "\n",
        "num_games = 1000\n",
        "wins_player1 = 0\n",
        "wins_player2 = 0\n",
        "ties = 0\n",
        "\n",
        "for _ in range(num_games):\n",
        "    outcome = play_game(random_player, random_player, verbose=False)\n",
        "    if outcome == +1:\n",
        "        wins_player1 += 1\n",
        "    elif outcome == -1:\n",
        "        wins_player2 += 1\n",
        "    else:\n",
        "        ties += 1\n",
        "\n",
        "print(f\"Over {num_games} random vs. random games:\")\n",
        "print(f\"Player 1 wins: {wins_player1}\")\n",
        "print(f\"Player 2 wins: {wins_player2}\")\n",
        "print(f\"Ties:          {ties}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Result Analysis\n",
        "\n",
        "P1 wins more than P2, with minimal ties, this is expected as player 1 has a great advantage compared to P2 in Connect 4.\n",
        "\n",
        "The win loss ratio for p1 vs p2 is about 60:40, which is expected. As with an optimal agent P1 always wins.\n",
        "\n",
        "Ties are interesting, as usually connect 4 always has a winner. So seeing ties is kind of interesting."
      ],
      "metadata": {
        "id": "Ob1kwiHurIsu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvzOxRDpL8cm"
      },
      "source": [
        "## Task 3: Minimax Search with Alpha-Beta Pruning [3 points]\n",
        "\n",
        "### Implement the search starting.\n",
        "\n",
        "Implement the search starting from a given state and specifying the player and put it into an agent function.\n",
        "You can use code from the [tic-tac-toe example](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_alpha_beta_tree_search.ipynb).\n",
        "\n",
        "__Notes:__\n",
        "* Make sure that all your agent functions have a signature consistent with the random agent above.\n",
        "* The search space for a $6 \\times 7$ board is large. You can experiment with smaller boards (the smallest is $4 \\times 4$) and/or changing the winning rule to connect 3 instead of 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "GJ2xQLrWL8cm"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def max_value_ab(board, player, last_move_was_mean, alpha, beta):\n",
        "    if terminal(board):\n",
        "        return utility(board), None\n",
        "    v, best_action = -math.inf, None\n",
        "    for a in actions(board, player, last_move_was_mean):\n",
        "        new_board, next_player, new_mean = result(board, a, player, last_move_was_mean)\n",
        "        v2, _ = min_value_ab(new_board, next_player, new_mean, alpha, beta)\n",
        "        if v2 > v:\n",
        "            v, best_action = v2, a\n",
        "        alpha = max(alpha, v)\n",
        "        if v >= beta:\n",
        "            break\n",
        "    return v, best_action\n",
        "\n",
        "def min_value_ab(board, player, last_move_was_mean, alpha, beta):\n",
        "    if terminal(board):\n",
        "        return utility(board), None\n",
        "    v, best_action = math.inf, None\n",
        "    for a in actions(board, player, last_move_was_mean):\n",
        "        new_board, next_player, new_mean = result(board, a, player, last_move_was_mean)\n",
        "        v2, _ = max_value_ab(new_board, next_player, new_mean, alpha, beta)\n",
        "        if v2 < v:\n",
        "            v, best_action = v2, a\n",
        "        beta = min(beta, v)\n",
        "        if v <= alpha:\n",
        "            break\n",
        "    return v, best_action\n",
        "def alpha_beta_agent(board, player, last_move_was_mean=False):\n",
        "    \"\"\"\n",
        "    If player=+1 => do max_value_ab\n",
        "    If player=-1 => do min_value_ab\n",
        "    Returns just the chosen action.\n",
        "    \"\"\"\n",
        "    alpha, beta = -math.inf, math.inf\n",
        "    if player == +1:\n",
        "        _, move = max_value_ab(board, player, last_move_was_mean, alpha, beta)\n",
        "    else:\n",
        "        _, move = min_value_ab(board, player, last_move_was_mean, alpha, beta)\n",
        "    return move\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQeK9qFXL8cm"
      },
      "source": [
        "Experiment with some manually created boards (at least 5) to check if the agent spots winning opportunities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ECcMh7WTL8cm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a401d9-0028-4334-de48-d99bdc5887ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current board state:\n",
            " [[ 0  0  0  0]\n",
            " [ 0  0  0  0]\n",
            " [ 0  0  0  0]\n",
            " [ 1  1  1 -1]]\n",
            "Player to move: Player +1\n",
            "last_move_was_mean: False\n",
            "Chosen action by alpha-beta: ('regular', 2)\n",
            "Time taken: 43.536741 seconds\n",
            "----------------------------------------\n",
            "Current board state:\n",
            " [[ 0  0  0  0]\n",
            " [ 0  0  0  0]\n",
            " [ 1  0 -1  0]\n",
            " [ 1  1 -1  0]]\n",
            "Player to move: Player -1\n",
            "last_move_was_mean: False\n",
            "Chosen action by alpha-beta: ('regular', 2)\n",
            "Time taken: 1.477536 seconds\n",
            "----------------------------------------\n",
            "Current board state:\n",
            " [[ 0  0  0  0]\n",
            " [ 0  1 -1  0]\n",
            " [ 0  1  1 -1]\n",
            " [ 1 -1  1 -1]]\n",
            "Player to move: Player +1\n",
            "last_move_was_mean: False\n",
            "Chosen action by alpha-beta: ('regular', 0)\n",
            "Time taken: 0.556321 seconds\n",
            "----------------------------------------\n",
            "Current board state:\n",
            " [[ 0  0  0  0]\n",
            " [ 0  0  0  0]\n",
            " [ 0  0  0  0]\n",
            " [ 1  0  0  0]\n",
            " [ 1 -1  0  0]\n",
            " [ 1 -1  0  0]]\n",
            "Player to move: Player +1\n",
            "last_move_was_mean: False\n",
            "Chosen action by alpha-beta: ('regular', 0)\n",
            "Time taken: 20.710795 seconds\n",
            "----------------------------------------\n",
            "Current board state:\n",
            " [[ 1  1 -1  1  0  0]\n",
            " [ 1 -1  1  1 -1  0]\n",
            " [ 1  1  1 -1  1 -1]\n",
            " [ 1 -1 -1  1  1 -1]]\n",
            "Player to move: Player -1\n",
            "last_move_was_mean: False\n",
            "Chosen action by alpha-beta: None\n",
            "Time taken: 0.000965 seconds\n",
            "----------------------------------------\n",
            "Finished testing 5 manually created boards.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "def experiment_alpha_beta(board, player=+1, last_move_was_mean=False):\n",
        "    \"\"\"\n",
        "    1) Print the board\n",
        "    2) Call alpha_beta_agent\n",
        "    3) Measure time\n",
        "    4) Print the chosen action\n",
        "    \"\"\"\n",
        "    print(\"Current board state:\\n\", board)\n",
        "    print(\"Player to move:\", \"Player +1\" if player == +1 else \"Player -1\")\n",
        "    print(\"last_move_was_mean:\", last_move_was_mean)\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "    chosen_action = alpha_beta_agent(board, player, last_move_was_mean)\n",
        "    end_time = time.perf_counter()\n",
        "\n",
        "    print(f\"Chosen action by alpha-beta: {chosen_action}\")\n",
        "    print(f\"Time taken: {end_time - start_time:.6f} seconds\")\n",
        "    print(\"-\"*40)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Smaller board first (4x4), for quick results\n",
        "\n",
        "    # 1) Simple board where +1 has an advantage\n",
        "    board1 = np.zeros((4,4), dtype=int)\n",
        "    board1[3,0] = +1\n",
        "    board1[3,1] = +1\n",
        "    board1[3,2] = +1\n",
        "    board1[3,3] = -1\n",
        "\n",
        "    experiment_alpha_beta(board1, player=+1, last_move_was_mean=False)\n",
        "\n",
        "    # 2) A board where -1 can do a mean move to disrupt +1\n",
        "    board2 = np.zeros((4,4), dtype=int)\n",
        "    board2[3,0] = +1\n",
        "    board2[3,1] = +1\n",
        "    board2[2,0] = +1\n",
        "    board2[3,2] = -1\n",
        "    board2[2,2] = -1\n",
        "    # last move was not mean => -1 can consider a mean move if there's a +1 at bottom somewhere\n",
        "    experiment_alpha_beta(board2, player=-1, last_move_was_mean=False)\n",
        "\n",
        "    # 3) A mid-game state (4x4) with some random discs\n",
        "    board3 = np.array([\n",
        "        [ 0,  0,  0,  0],\n",
        "        [ 0, +1, -1,  0],\n",
        "        [ 0, +1, +1, -1],\n",
        "        [+1, -1, +1, -1]\n",
        "    ])\n",
        "    experiment_alpha_beta(board3, player=+1, last_move_was_mean=False)\n",
        "\n",
        "    # 4) Another state with potential vertical connect-4\n",
        "    board4 = np.zeros((6,4), dtype=int)\n",
        "    # place +1 vertically in col=0\n",
        "    board4[5,0] = +1\n",
        "    board4[4,0] = +1\n",
        "    board4[3,0] = +1\n",
        "    # Place -1 somewhere else\n",
        "    board4[5,1] = -1\n",
        "    board4[4,1] = -1\n",
        "    experiment_alpha_beta(board4, player=+1, last_move_was_mean=False)\n",
        "\n",
        "    # 5) Larger board, near full. 4x6\n",
        "    board5 = np.array([\n",
        "        [+1, +1, -1, +1,  0,  0],\n",
        "        [+1, -1, +1, +1, -1,  0],\n",
        "        [+1, +1, +1, -1, +1, -1],\n",
        "        [+1, -1, -1, +1, +1, -1]\n",
        "    ])\n",
        "    # Suppose it's -1's turn\n",
        "    experiment_alpha_beta(board5, player=-1, last_move_was_mean=False)\n",
        "\n",
        "    print(\"Finished testing 5 manually created boards.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ByI_b2VL8cm"
      },
      "source": [
        "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Answer\n",
        "\n",
        "It takes on average 20-40 seconds given a clean board, and grows exponentially given the increase in possible moves."
      ],
      "metadata": {
        "id": "xOzmnMu1znMD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDHTe0LcL8cn"
      },
      "source": [
        "### Move ordering\n",
        "\n",
        "Starting the search with better moves will increase the efficiency of alpha-beta pruning. Describe and implement a simple move ordering strategy. Make a table that shows how the ordering strategies influence the time it takes to make a move."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "omeYo7HyL8cn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0b91e64-e461-4deb-8249-64aa63bfc33e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Board Size | MoveOrder=False Time (s) | MoveOrder=True Time (s) | Example Move\n",
            "----------------------------------------------------------------------\n",
            "4x4        |                81.189646 |               82.100465 | ('regular', 1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def ordered_actions(board, player, last_move_was_mean=False):\n",
        "    \"\"\"\n",
        "    Like 'actions', but we reorder columns so that columns near the center\n",
        "    are explored first. This tends to improve alpha-beta pruning in Connect4.\n",
        "    \"\"\"\n",
        "    acts = actions(board, player, last_move_was_mean)\n",
        "    rows, cols = board.shape\n",
        "    center = (cols - 1)/2.0\n",
        "\n",
        "    def priority(move):\n",
        "        (mt, c) = move\n",
        "        # more negative the distance => higher priority\n",
        "        # we can just do negative absolute distance\n",
        "        return -abs(c - center)\n",
        "\n",
        "    acts.sort(key=priority, reverse=True)\n",
        "\n",
        "    return acts\n",
        "\n",
        "\n",
        "#Reusing existing function names out of laziness\n",
        "def max_value_ab(board, player, last_move_was_mean, alpha, beta, move_order=False):\n",
        "    if terminal(board):\n",
        "        return utility(board), None\n",
        "    v, best_action = -math.inf, None\n",
        "\n",
        "    # Decide which actions function to use\n",
        "    if move_order:\n",
        "        possible_moves = ordered_actions(board, player, last_move_was_mean)\n",
        "    else:\n",
        "        possible_moves = actions(board, player, last_move_was_mean)\n",
        "\n",
        "    for a in possible_moves:\n",
        "        new_board, next_p, next_m = result(board, a, player, last_move_was_mean)\n",
        "        v2, _ = min_value_ab(new_board, next_p, next_m, alpha, beta, move_order)\n",
        "        if v2 > v:\n",
        "            v, best_action = v2, a\n",
        "        alpha = max(alpha, v)\n",
        "        if v >= beta:\n",
        "            break\n",
        "    return v, best_action\n",
        "\n",
        "def min_value_ab(board, player, last_move_was_mean, alpha, beta, move_order=False):\n",
        "    if terminal(board):\n",
        "        return utility(board), None\n",
        "    v, best_action = math.inf, None\n",
        "\n",
        "    if move_order:\n",
        "        possible_moves = ordered_actions(board, player, last_move_was_mean)\n",
        "    else:\n",
        "        possible_moves = actions(board, player, last_move_was_mean)\n",
        "\n",
        "    for a in possible_moves:\n",
        "        new_board, next_p, next_m = result(board, a, player, last_move_was_mean)\n",
        "        v2, _ = max_value_ab(new_board, next_p, next_m, alpha, beta, move_order)\n",
        "        if v2 < v:\n",
        "            v, best_action = v2, a\n",
        "        beta = min(beta, v)\n",
        "        if v <= alpha:\n",
        "            break\n",
        "    return v, best_action\n",
        "\n",
        "def alpha_beta_agent(board, player, last_move_was_mean=False, move_order=False):\n",
        "    alpha, beta = -math.inf, math.inf\n",
        "    if player == +1:\n",
        "        val, move = max_value_ab(board, player, last_move_was_mean, alpha, beta, move_order)\n",
        "    else:\n",
        "        val, move = min_value_ab(board, player, last_move_was_mean, alpha, beta, move_order)\n",
        "    return move\n",
        "\n",
        "def play_once(board, player, move_order=False):\n",
        "    \"\"\"\n",
        "    Play one move using alpha-beta, measure how long it takes.\n",
        "    Return (chosen_move, time_elapsed).\n",
        "    \"\"\"\n",
        "    start_t = time.perf_counter()\n",
        "    mv = alpha_beta_agent(board, player, move_order=move_order)\n",
        "    end_t = time.perf_counter()\n",
        "    elapsed = end_t - start_t\n",
        "    return mv, elapsed\n",
        "\n",
        "def make_experiment_table():\n",
        "    \"\"\"\n",
        "    We'll test a few board shapes or states.\n",
        "    For each, we measure the time to pick a single move\n",
        "    with (move_order=False) and with (move_order=True).\n",
        "    We'll store or print the results in a small table.\n",
        "    \"\"\"\n",
        "    # Example board sizes or states to measure\n",
        "    test_cases = []\n",
        "    # 1) 4x4 almost empty\n",
        "    test_cases.append( (np.zeros((4,4), dtype=int), +1) )\n",
        "\n",
        "    print(\"Board Size | MoveOrder=False Time (s) | MoveOrder=True Time (s) | Example Move\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    for board, ply in test_cases:\n",
        "        # with no ordering\n",
        "        _, t_no_order = play_once(board, ply, move_order=False)\n",
        "        # with ordering\n",
        "        chosen_move, t_order = play_once(board, ply, move_order=True)\n",
        "\n",
        "        shape_str = f\"{board.shape[0]}x{board.shape[1]}\"\n",
        "        print(f\"{shape_str:<10} | {t_no_order:24.6f} | {t_order:23.6f} | {chosen_move}\")\n",
        "make_experiment_table()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_ZJDd9nL8cn"
      },
      "source": [
        "### The first few moves\n",
        "\n",
        "Start with an empty board. This is the worst case scenario for minimax search with alpha-beta pruning since it needs solve all possible games that can be played (minus some pruning) before making the decision. What can you do?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Answer**\n",
        "To minimize the number of Extraneous moves, we can use a Depth Limit. Which, at the cost of accuracy can help save time in searching."
      ],
      "metadata": {
        "id": "rpaLFI3v2Y8k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhk1YRnDL8cn"
      },
      "source": [
        "### Playtime\n",
        "\n",
        "Let the Minimax Search agent play a random agent on a small board. Analyze wins, losses and draws."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Kx1qffv2L8co",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ce4ac1-c64c-4a0e-f9ad-852c577a8da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After 5 games on a 4x4 board:\n",
            "  Minimax (Player +1) wins: 5\n",
            "  Random  (Player -1) wins: 0\n",
            "  Ties: 0\n"
          ]
        }
      ],
      "source": [
        "#Redefining Play Game here to make size of board dynamic. Too laze to actually change original\n",
        "def play_game(agent1, agent2, rows=4, cols=4, verbose=False):\n",
        "    \"\"\"\n",
        "    Let agent1 (Player +1) and agent2 (Player -1) play on a board of size rows x cols.\n",
        "    Return the final utility: +1 if agent1 wins, -1 if agent2 wins, 0 if tie.\n",
        "    \"\"\"\n",
        "    board = np.zeros((rows, cols), dtype=int)\n",
        "    current_player = +1\n",
        "    last_move_mean = False\n",
        "\n",
        "    while True:\n",
        "        if terminal(board):\n",
        "            break\n",
        "        if current_player == +1:\n",
        "            action = agent1(board, +1, last_move_mean)\n",
        "        else:\n",
        "            action = agent2(board, -1, last_move_mean)\n",
        "\n",
        "        if not action:\n",
        "            # no legal moves => terminal\n",
        "            break\n",
        "\n",
        "        new_board, next_player, new_mean = result(board, action, current_player, last_move_mean)\n",
        "        board = new_board\n",
        "        current_player = next_player\n",
        "        last_move_mean = new_mean\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Player { -current_player } took action {action}\")\n",
        "            print(board, \"\\n\")\n",
        "\n",
        "    return utility(board)\n",
        "\n",
        "def experiment_minimax_vs_random(num_games=5, rows=4, cols=4):\n",
        "    \"\"\"\n",
        "    Let the alpha_beta_agent (or your minimax agent) face the random_agent\n",
        "    for num_games. Print how many wins, losses, ties.\n",
        "    \"\"\"\n",
        "    wins_p1 = 0\n",
        "    wins_p2 = 0\n",
        "    ties    = 0\n",
        "\n",
        "    for _ in range(num_games):\n",
        "        outcome = play_game(alpha_beta_agent, random_player, rows=rows, cols=cols, verbose=False)\n",
        "        if outcome == +1:\n",
        "            wins_p1 += 1\n",
        "        elif outcome == -1:\n",
        "            wins_p2 += 1\n",
        "        else:\n",
        "            ties += 1\n",
        "\n",
        "    print(f\"After {num_games} games on a {rows}x{cols} board:\")\n",
        "    print(f\"  Minimax (Player +1) wins: {wins_p1}\")\n",
        "    print(f\"  Random  (Player -1) wins: {wins_p2}\")\n",
        "    print(f\"  Ties: {ties}\")\n",
        "\n",
        "experiment_minimax_vs_random()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NOTE\n",
        "This part is what made me late. I tried to run 5 games, this took 16 minutes per game. But originally took 80... I need to either fix the runtime on this thing or use a local next time.\n",
        "\n",
        "The end result was that Minimax won all of the games.\n",
        "\n",
        "This is pretty expected as Minimax is a proper algorithm that can take advantage of its opponent's misplays."
      ],
      "metadata": {
        "id": "oLA1Evrf_cTP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXE2i58iL8co"
      },
      "source": [
        "## Task 4: Heuristic Alpha-Beta Tree Search [3 points]\n",
        "\n",
        "### Heuristic evaluation function\n",
        "\n",
        "Define and implement a heuristic evaluation function."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Heuristic Evaluation Function would implement the usage of Heuristics, or some calculated score value that the Agent would use to \"maximize\" its outputs.\n",
        "\n",
        "As in\n",
        "\n",
        " Example scoring :\n",
        "\n",
        "    +100 for 4 in a row\n",
        "\n",
        "    +50 for 3 in a row + 1 empty\n",
        "\n",
        "    +10 for 2 in a row + 2 empty\n",
        "\n",
        "    +1 for 1 in a row + 3 empty\n",
        "\n",
        "    0 otherwise"
      ],
      "metadata": {
        "id": "d8XVP09K6Q8g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbIviqR6L8co"
      },
      "source": [
        "### Cutting off search\n",
        "\n",
        "Modify your Minimax Search with Alpha-Beta Pruning to cut off search at a specified depth and use the heuristic evaluation function. Experiment with different cutoff values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "IhKCwDTPL8co"
      },
      "outputs": [],
      "source": [
        "def evaluate_window(window, player):\n",
        "    \"\"\"\n",
        "    Given a 1D array/list of length 4 (the \"window\"),\n",
        "    return a score indicating how good this window is for 'player'.\n",
        "\n",
        "    \"\"\"\n",
        "    # Count occurrences\n",
        "    count_self = np.count_nonzero(window == player)\n",
        "    count_opp  = np.count_nonzero(window == -player)\n",
        "    count_empty = np.count_nonzero(window == 0)\n",
        "\n",
        "    # If window contains both player's discs => worthless to either\n",
        "    if count_self > 0 and count_opp > 0:\n",
        "        return 0\n",
        "\n",
        "    # Score lines that are exclusively ours or exclusively empty\n",
        "    if count_self == 4:\n",
        "        return 10000\n",
        "    elif count_self == 3 and count_empty == 1:\n",
        "        return 50\n",
        "    elif count_self == 2 and count_empty == 2:\n",
        "        return 10\n",
        "    elif count_self == 1 and count_empty == 3:\n",
        "        return 1\n",
        "\n",
        "    return 0\n",
        "\n",
        "def heuristic_evaluation(board):\n",
        "    \"\"\"\n",
        "    Returns a heuristic value indicating how good the board is for each player\n",
        "\n",
        "    \"\"\"\n",
        "    rows, cols = board.shape\n",
        "    score_p1 = 0\n",
        "    score_p2 = 0\n",
        "\n",
        "    # --- Horizontal ---\n",
        "    for r in range(rows):\n",
        "        for c in range(cols - 3):\n",
        "            window = board[r, c:c+4]\n",
        "            score_p1 += evaluate_window(window, +1)\n",
        "            score_p2 += evaluate_window(window, -1)\n",
        "\n",
        "    # --- Vertical ---\n",
        "    for r in range(rows - 3):\n",
        "        for c in range(cols):\n",
        "            window = board[r:r+4, c]\n",
        "            score_p1 += evaluate_window(window, +1)\n",
        "            score_p2 += evaluate_window(window, -1)\n",
        "\n",
        "    # --- Diagonal (down-right) ---\n",
        "    for r in range(rows - 3):\n",
        "        for c in range(cols - 3):\n",
        "            window = [board[r+i, c+i] for i in range(4)]\n",
        "            score_p1 += evaluate_window(window, +1)\n",
        "            score_p2 += evaluate_window(window, -1)\n",
        "\n",
        "    # --- Diagonal (down-left) ---\n",
        "    for r in range(rows - 3):\n",
        "        for c in range(3, cols):\n",
        "            window = [board[r+i, c-i] for i in range(4)]\n",
        "            score_p1 += evaluate_window(window, +1)\n",
        "            score_p2 += evaluate_window(window, -1)\n",
        "\n",
        "    return score_p1 - score_p2\n",
        "\n",
        "def alpha_beta_search(board, player, last_move_was_mean, alpha, beta, depth, max_depth):\n",
        "    # 1) Terminal test\n",
        "    if terminal(board):\n",
        "        # Return actual utility if it's truly terminal\n",
        "        return utility(board), None\n",
        "    # 2) Depth cutoff\n",
        "    if depth >= max_depth:\n",
        "        # Return heuristic eval instead of True utility\n",
        "        return heuristic_evaluation(board), None\n",
        "\n",
        "    # 3) Normal alpha-beta logic...\n",
        "    if player == +1:\n",
        "        return max_value_ab(board, player, last_move_was_mean, alpha, beta, depth, max_depth)\n",
        "    else:\n",
        "        return min_value_ab(board, player, last_move_was_mean, alpha, beta, depth, max_depth)\n",
        "\n",
        "def max_value_ab(board, player, last_move_was_mean, alpha, beta, depth, max_depth):\n",
        "    v = -float('inf')\n",
        "    best_move = None\n",
        "    possible_moves = actions(board, player, last_move_was_mean)\n",
        "    if not possible_moves:\n",
        "        return heuristic_evaluation(board), None\n",
        "    for a in possible_moves:\n",
        "        new_board, next_player, new_mean = result(board, a, player, last_move_was_mean)\n",
        "        v2, _ = alpha_beta_search(new_board, next_player, new_mean, alpha, beta, depth+1, max_depth)\n",
        "        if v2 > v:\n",
        "            v = v2\n",
        "            best_move = a\n",
        "        alpha = max(alpha, v)\n",
        "        if alpha >= beta:\n",
        "            break\n",
        "    return v, best_move\n",
        "\n",
        "def min_value_ab(board, player, last_move_was_mean, alpha, beta, depth, max_depth):\n",
        "    v = float('inf')\n",
        "    best_move = None\n",
        "    possible_moves = actions(board, player, last_move_was_mean)\n",
        "    if not possible_moves:\n",
        "        return heuristic_evaluation(board), None\n",
        "    for a in possible_moves:\n",
        "        new_board, next_player, new_mean = result(board, a, player, last_move_was_mean)\n",
        "        v2, _ = alpha_beta_search(new_board, next_player, new_mean, alpha, beta, depth+1, max_depth)\n",
        "        if v2 < v:\n",
        "            v = v2\n",
        "            best_move = a\n",
        "        beta = min(beta, v)\n",
        "        if beta <= alpha:\n",
        "            break\n",
        "    return v, best_move\n",
        "\n",
        "def heuristic_alpha_beta_agent(board, player, last_move_was_mean=False, max_depth=4):\n",
        "    alpha, beta = -float('inf'), float('inf')\n",
        "    best_val, best_move = alpha_beta_search(board, player, last_move_was_mean, alpha, beta, depth=0, max_depth=max_depth)\n",
        "    return best_move"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBpYGvxBL8co"
      },
      "source": [
        "Experiment with the same manually created boards as above to check if the agent spots winning opportunities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "KHqb_K5YL8co",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d576510b-fd13-467d-e2ed-16e986866c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Board:\n",
            " [[ 0  0  0  0]\n",
            " [ 0  0  0  0]\n",
            " [ 0  0  0  0]\n",
            " [ 1  1  1 -1]]\n",
            "Player to move: 1, last_move_mean=False\n",
            "Heuristic Alpha-Beta chooses: ('regular', 3)\n",
            "Time taken: 0.069179 seconds\n",
            "--------------------------------------------------\n",
            "Board:\n",
            " [[ 0  0  0  0]\n",
            " [ 0  0  0  0]\n",
            " [ 1  0 -1  0]\n",
            " [ 1  1 -1  0]]\n",
            "Player to move: -1, last_move_mean=False\n",
            "Heuristic Alpha-Beta chooses: ('regular', 0)\n",
            "Time taken: 0.025680 seconds\n",
            "--------------------------------------------------\n",
            "Board:\n",
            " [[ 0  0  0  0]\n",
            " [ 0  1 -1  0]\n",
            " [ 0  1  1 -1]\n",
            " [ 1 -1  1 -1]]\n",
            "Player to move: 1, last_move_mean=False\n",
            "Heuristic Alpha-Beta chooses: ('regular', 0)\n",
            "Time taken: 0.094388 seconds\n",
            "--------------------------------------------------\n",
            "Board:\n",
            " [[ 0  0  0  0]\n",
            " [ 0  0  0  0]\n",
            " [ 0  0  0  0]\n",
            " [ 1  0  0  0]\n",
            " [ 1 -1  0  0]\n",
            " [ 1 -1  0  0]]\n",
            "Player to move: 1, last_move_mean=False\n",
            "Heuristic Alpha-Beta chooses: ('regular', 1)\n",
            "Time taken: 0.144255 seconds\n",
            "--------------------------------------------------\n",
            "Board:\n",
            " [[ 1  1 -1  1  0  0]\n",
            " [ 1 -1  1  1 -1  0]\n",
            " [ 1  1  1 -1  1 -1]\n",
            " [ 1 -1 -1  1  1 -1]]\n",
            "Player to move: -1, last_move_mean=False\n",
            "Heuristic Alpha-Beta chooses: None\n",
            "Time taken: 0.000434 seconds\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def test_heuristic_on_board(board, player, last_move_mean=False, max_depth=4):\n",
        "    \"\"\"\n",
        "    1) Print the board\n",
        "    2) Run the heuristic alpha-beta agent\n",
        "    3) Print the chosen action and time\n",
        "    \"\"\"\n",
        "    print(\"Board:\\n\", board)\n",
        "    print(f\"Player to move: {player}, last_move_mean={last_move_mean}\")\n",
        "\n",
        "    start_t = time.perf_counter()\n",
        "    move = heuristic_alpha_beta_agent(board, player, last_move_mean, max_depth)\n",
        "    end_t = time.perf_counter()\n",
        "\n",
        "    print(f\"Heuristic Alpha-Beta chooses: {move}\")\n",
        "    print(f\"Time taken: {end_t - start_t:.6f} seconds\\n{'-'*50}\")\n",
        "\n",
        "\n",
        "board1 = np.zeros((4,4), dtype=int)\n",
        "board1[3,0] = +1\n",
        "board1[3,1] = +1\n",
        "board1[3,2] = +1\n",
        "board1[3,3] = -1\n",
        "\n",
        "test_heuristic_on_board(board1, player=+1, max_depth=4)\n",
        "\n",
        "\n",
        "board2 = np.zeros((4,4), dtype=int)\n",
        "board2[3,0] = +1\n",
        "board2[3,1] = +1\n",
        "board2[2,0] = +1\n",
        "board2[3,2] = -1\n",
        "board2[2,2] = -1\n",
        "test_heuristic_on_board(board2, player=-1, max_depth=4)\n",
        "\n",
        "board3 = np.array([\n",
        "    [ 0,  0,  0,  0],\n",
        "    [ 0, +1, -1,  0],\n",
        "    [ 0, +1, +1, -1],\n",
        "    [+1, -1, +1, -1]\n",
        "])\n",
        "test_heuristic_on_board(board3, player=+1, max_depth=5)\n",
        "\n",
        "board4 = np.zeros((6,4), dtype=int)\n",
        "board4[5,0] = +1\n",
        "board4[4,0] = +1\n",
        "board4[3,0] = +1\n",
        "board4[5,1] = -1\n",
        "board4[4,1] = -1\n",
        "test_heuristic_on_board(board4, player=+1, max_depth=5)\n",
        "\n",
        "board5 = np.array([\n",
        "    [+1, +1, -1, +1,  0,  0],\n",
        "    [+1, -1, +1, +1, -1,  0],\n",
        "    [+1, +1, +1, -1, +1, -1],\n",
        "    [+1, -1, -1, +1, +1, -1]\n",
        "])\n",
        "test_heuristic_on_board(board5, player=-1, max_depth=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaBC7W7VL8cp"
      },
      "source": [
        "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It takes a minimal amount of time for it to perform most of its tasks"
      ],
      "metadata": {
        "id": "Yoy0VMrk8uyH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bokyrXrkL8cp"
      },
      "source": [
        "### Playtime\n",
        "\n",
        "Let two heuristic search agents (different cutoff depth, different heuristic evaluation function) compete against each other on a reasonably sized board. Since there is no randomness, you only need to let them play once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "hUILjLbdL8cp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a9f62e5-5bbf-4f23-ffc6-d0987fcfb74b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Move 1. Player 1 did: ('regular', 3)\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]] \n",
            "\n",
            "Move 2. Player -1 did: ('regular', 3)\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]] \n",
            "\n",
            "Move 3. Player 1 did: ('regular', 3)\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]] \n",
            "\n",
            "Move 4. Player -1 did: ('regular', 2)\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0]\n",
            " [ 0  0 -1  1  0  0  0]] \n",
            "\n",
            "Move 5. Player 1 did: ('regular', 3)\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0]\n",
            " [ 0  0 -1  1  0  0  0]] \n",
            "\n",
            "Move 6. Player -1 did: ('regular', 4)\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0]\n",
            " [ 0  0 -1  1 -1  0  0]] \n",
            "\n",
            "Move 7. Player 1 did: ('regular', 2)\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  1 -1  0  0  0]\n",
            " [ 0  0 -1  1 -1  0  0]] \n",
            "\n",
            "Move 8. Player -1 did: ('regular', 5)\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  1 -1  0  0  0]\n",
            " [ 0  0 -1  1 -1 -1  0]] \n",
            "\n",
            "Move 9. Player 1 did: ('regular', 3)\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  1 -1  0  0  0]\n",
            " [ 0  0 -1  1 -1 -1  0]] \n",
            "\n",
            "Move 10. Player -1 did: ('mean', 3)\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  1  1  0  0  0]\n",
            " [ 0  0 -1 -1 -1 -1  0]] \n",
            "\n",
            "It's a tie!\n",
            "Final board:\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  1  1  0  0  0]\n",
            " [ 0  0 -1 -1 -1 -1  0]]\n",
            "Total time: 209.610 seconds\n"
          ]
        }
      ],
      "source": [
        "def play_game(agent1, agent2, rows=6, cols=7, verbose=False):\n",
        "    board = np.zeros((rows, cols), dtype=int)\n",
        "    current_player = +1\n",
        "    last_mean = False\n",
        "    move_count = 0\n",
        "\n",
        "    while not terminal(board):\n",
        "        if current_player == +1:\n",
        "            action = heuristic_alpha_beta_agent(board, +1, last_mean,4)\n",
        "        else:\n",
        "            action = heuristic_alpha_beta_agent(board, -1, last_mean,6)\n",
        "\n",
        "        if not action:\n",
        "            # no moves => must be terminal\n",
        "            break\n",
        "\n",
        "        new_board, next_p, new_mean = result(board, action, current_player, last_mean)\n",
        "        board = new_board\n",
        "        current_player = next_p\n",
        "        last_mean = new_mean\n",
        "        move_count += 1\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Move {move_count}. Player { -current_player } did: {action}\")\n",
        "            print(board, \"\\n\")\n",
        "\n",
        "    final_util = utility(board)\n",
        "    if final_util == +1:\n",
        "        print(\"Agent A (depth=4) wins!\")\n",
        "    elif final_util == -1:\n",
        "        print(\"Agent B (depth=6) wins!\")\n",
        "    else:\n",
        "        print(\"It's a tie!\")\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Final board:\")\n",
        "        print(board)\n",
        "if __name__ == \"__main__\":\n",
        "    start_t = time.perf_counter()\n",
        "    play_game(heuristic_alpha_beta_agent, heuristic_alpha_beta_agent, rows=6, cols=7, verbose=True)\n",
        "    end_t = time.perf_counter()\n",
        "    print(f\"Total time: {end_t - start_t:.3f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFhDH9M_L8cp"
      },
      "source": [
        "---\n",
        "Assignment adapted from [Michael Hahsler](https://github.com/mhahsler/CS7320-AI) under [CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0/deed.en) license.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}