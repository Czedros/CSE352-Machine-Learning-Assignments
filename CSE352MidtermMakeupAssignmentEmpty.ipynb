{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Czedros/CSE352-Machine-Learning-Assignments/blob/main/CSE352MidtermMakeupAssignmentEmpty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Introduction**\n",
        "\n",
        "Name: [Enter Here]\n",
        "\n",
        "I understand that my submission needs to be my own work: [Initials]\n",
        "\n",
        "I understand that ChatGPT / Copilot / other AI tools are not allowed:  [Initials]\n",
        "\n",
        "Total Points: **X**\n",
        "\n",
        "Complete this notebook and submit it (save/print as pdf). Make sure all output is correct in the pdf before submitting (it sometimes gets cut off).\n",
        "\n",
        " The notebook needs to be a complete project report with your implementation, documentation including a short discussion of how your implementation works and your design choices, and experimental results (e.g., tables and charts with simulation results) with a short discussion of what they mean. Use the provided notebook cells and insert additional code and markdown cells as needed."
      ],
      "metadata": {
        "id": "or-AfmlaFBvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview**\n",
        "\n",
        "In this assignment, you will:\n",
        "- Implement the rules and structure for Ultimate Tic-Tac-Toe\n",
        "- Design a Monte Carlo Tree Search (MCTS) agent\n",
        "- Improve your Monto Carlo Tree Search Agent\n",
        "- Defeat a Cohort of Challengers with your agent.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jlbGPJMxmWXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ultimate Tic-Tac-Toe**\n",
        "\n",
        "Ultimate Tic-Tac-Toe is a strategic extension of the classic Tic-Tac-Toe game. Rather than a single 3×3 grid, Ultimate Tic-Tac-Toe consists of a 3×3 grid where each cell contains another 3×3 Tic-Tac-Toe board. This layered setup creates a more complex game structure requiring players to think multiple moves ahead.\n",
        "\n",
        "The game was first popularized online in the early 2010s and is designed to emphasize positional play, long-term strategy, and adaptation to an evolving board state. Each player, X or O, takes turns placing their mark in a cell of one of the smaller boards. However, the twist lies in the move constraints: a player’s move determines which of the nine small boards their opponent must play in next. Specifically, if a player places their mark in cell (i, j) of a small board, their opponent must play in the (i, j) small board on their next turn.\n",
        "\n",
        "The objective is to win the larger game by winning three small boards in a row—horizontally, vertically, or diagonally—similar to classic Tic-Tac-Toe. A small board is won when a player has three marks in a row within it. If a player is sent to a board that is already won or drawn, they may play in any open board of their choosing.\n",
        "\n",
        "Ultimate Tic-Tac-Toe presents a more intricate challenge than its predecessor and is a rich environment for studying game trees, simulation-based planning, and reinforcement learning.\n",
        "\n",
        "Additional Reading:\n",
        "\n",
        "[Wikipedia page](https://en.wikipedia.org/wiki/Ultimate_tic-tac-toe).\n",
        "\n",
        "\n",
        "[Quick Video Demonstration on how the Game works](https://www.youtube.com/shorts/_Na3a1ZrX7c) (Credit: VSauce)\n",
        "\n",
        "[More In-Depth Guide](https://www.youtube.com/watch?v=37PC0bGMiTI) (Credit: MindYourDecisions - Presh Talwalkar)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dYcSRE2jmWMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualizer Helper\n",
        "\n",
        "There are 2 ways to go about creating a board for Ultimate Tic Tac Toe, a 9x9 grid or a 3x3 of 3x3 grids. These will result in different ways to visualize the board.\n",
        "\n",
        "\n",
        "The following is a visuzlizing tool and 2 adapters that can be used depending on your implementation of the game.\n",
        "\n",
        "#Implementation 1: 9x9 Boards"
      ],
      "metadata": {
        "id": "dvY0Xy_SO54c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def draw_ultimate_board(board, small_board_winners=None, last_move=None, highlight=None):\n",
        "    \"\"\"\n",
        "    Visualizes a 9x9 Ultimate Tic-Tac-Toe board with enhancements:\n",
        "    - Grays out small boards that have been won\n",
        "    - Draws a large X or O over won small boards\n",
        "    - Highlights last move in bold\n",
        "    - Red border for active small board\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(6,6))\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xlim(0, 9)\n",
        "    ax.set_ylim(0, 9)\n",
        "\n",
        "    # Draw grid\n",
        "    for i in range(10):\n",
        "        lw = 2 if i % 3 == 0 else 0.5\n",
        "        ax.axhline(i, color='black', lw=lw, zorder=0)\n",
        "        ax.axvline(i, color='black', lw=lw, zorder=0)\n",
        "\n",
        "    # Draw faded backgrounds for won small boards\n",
        "    if small_board_winners:\n",
        "        for br in range(3):\n",
        "            for bc in range(3):\n",
        "                winner = small_board_winners[br][bc]\n",
        "                if winner in [1, 2]:\n",
        "                    rect = plt.Rectangle((bc*3, 6 - br*3), 3, 3, color='gray', alpha=0.6, zorder=1)\n",
        "                    ax.add_patch(rect)\n",
        "                    ax.text(bc*3 + 1.5, 7.5 - br*3, 'X' if winner == 1 else 'O',\n",
        "                            fontsize=80, ha='center', va='center', color='black', alpha=1, zorder=2)\n",
        "\n",
        "    # Draw pieces\n",
        "    for row in range(9):\n",
        "        for col in range(9):\n",
        "            val = board[row, col]\n",
        "            if val == 1:\n",
        "                ax.text(col+0.5, 8.5 - row, 'X', ha='center', va='center', fontsize=16,\n",
        "                        fontweight='bold' if (last_move == (row, col)) else 'normal', zorder=3)\n",
        "            elif val == 2:\n",
        "                ax.text(col+0.5, 8.5 - row, 'O', ha='center', va='center', fontsize=16,\n",
        "                        fontweight='bold' if (last_move == (row, col)) else 'normal', zorder=3)\n",
        "\n",
        "    # Highlight active small board\n",
        "    if highlight:\n",
        "        hrow, hcol = highlight\n",
        "        x0 = hcol * 3\n",
        "        y0 = 6 - hrow * 3\n",
        "        ax.plot([x0, x0+3], [y0, y0], color='red', lw=4, zorder=4)\n",
        "        ax.plot([x0, x0+3], [y0+3, y0+3], color='red', lw=4, zorder=4)\n",
        "        ax.plot([x0, x0], [y0, y0+3], color='red', lw=4, zorder=4)\n",
        "        ax.plot([x0+3, x0+3], [y0, y0+3], color='red', lw=4, zorder=4)\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "q43e5_I5zFSN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##An example of the visualizer code and how its used"
      ],
      "metadata": {
        "id": "JQPSrO8uRTTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_board = np.zeros((9, 9), dtype=int)\n",
        "test_board[0, 0] = 1\n",
        "test_board[1, 1] = 1\n",
        "test_board[2, 2] = 1\n",
        "test_board[3, 3] = 2\n",
        "test_board[4, 4] = 2\n",
        "test_board[5, 5] = 2\n",
        "\n",
        "small_board_winners = [\n",
        "    [1, 0, 0],\n",
        "    [0, 2, 0],\n",
        "    [0, 0, 0]\n",
        "]\n",
        "\n",
        "last_move = (5, 5)\n",
        "highlight = (1, 1)\n",
        "\n",
        "draw_ultimate_board(test_board, small_board_winners, last_move, highlight)"
      ],
      "metadata": {
        "id": "k96Fi8lZRSlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adapter 1: Using 3x3 of 3x3 arrays"
      ],
      "metadata": {
        "id": "PMJkNCWR_2_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nested_list_to_board(nested_game, highlight=None, last_move=None, winners=None):\n",
        "    board_array = np.zeros((9, 9), dtype=int)\n",
        "    for br in range(3):\n",
        "        for bc in range(3):\n",
        "            sb = nested_game[br][bc]\n",
        "            if isinstance(sb, np.ndarray):\n",
        "                board_array[br*3:(br+1)*3, bc*3:(bc+1)*3] = sb\n",
        "            else:\n",
        "                board_array[br*3:(br+1)*3, bc*3:(bc+1)*3] = np.array(sb)\n",
        "\n",
        "    draw_ultimate_board(board_array, small_board_winners=winners, last_move=last_move, highlight=highlight)"
      ],
      "metadata": {
        "id": "5xbUi3CzsPBK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example Usage"
      ],
      "metadata": {
        "id": "Jem1Y1UA_6Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nested_game = [\n",
        "    [np.array([[1,0,0],[0,1,0],[0,0,1]]), np.zeros((3,3)), np.zeros((3,3))],\n",
        "    [np.zeros((3,3)), np.array([[2,0,0],[0,2,0],[0,0,2]]), np.zeros((3,3))],\n",
        "    [np.zeros((3,3)), np.zeros((3,3)), np.zeros((3,3))]\n",
        "]\n",
        "\n",
        "winners = [\n",
        "    [1, 0, 0],\n",
        "    [0, 2, 0],\n",
        "    [0, 0, 0]\n",
        "]\n",
        "\n",
        "nested_list_to_board(nested_game, highlight=(0,0), last_move=(0,0), winners=winners)"
      ],
      "metadata": {
        "id": "veMyisXgrkps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1: Ultimate Tic-Tac-Toe [X Points]\n",
        "In the first part, you will implement the mechanics of Ultimate Tic-Tac-Toe, defining how the game progresses and determining legal moves and win conditions.\n",
        "\n",
        "There are an infinite number of ways to do this, and the guidelines will be pretty Lax\n",
        "\n",
        "Here are two reccomended Methods and the Specifications\n",
        "\n",
        "##Method 1: 3x3 Implementation\n",
        "\n",
        "- Create a Small Board Class that contains a basic Tic-Tac-Toe Game\n",
        "- Create a Big Board Class that contains a 3x3 grid of Small boards\n",
        "- Implement logic for Tic tac toe..\n",
        "\n",
        "##Method 2: Loose 9x9 Implementation\n",
        "- Make a 9x9 grid\n",
        "- Whenever a Move is made, Check for legality of the move and make it\n",
        "- Check for wins via checking every 3x3 in code.\n",
        "\n",
        "##Requirements\n",
        "\n",
        "Regardless of the methods, several things are mandatory.\n",
        "\n",
        "- get_legal_moves(self) : Gives a list of legal plays (This is important for the bots!)\n",
        "\n",
        "- use big_r, big_c, small_r, small_c as the format of the moves (This is important for the bots!)\n",
        "\n",
        "\n",
        "(Alternatively, give this to students as is, and have them complete just the bots,those are alot of work already, and this was pretty simple)"
      ],
      "metadata": {
        "id": "PFkDGV_BObd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write Your Code Here"
      ],
      "metadata": {
        "id": "C_uNRwgHpXFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test 1: Random Bot\n",
        "\n",
        "Implement a bot that makes Random Moves, and have it play against itself! Use this to test out whether your game is working correctly, and see how the game plays.\n"
      ],
      "metadata": {
        "id": "q0SJ1cfEXVZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write Your Code Here"
      ],
      "metadata": {
        "id": "WZYW0hW-wHC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 2: Monte Carlo Tree Search [X Points]**\n",
        "\n",
        "The second part of this assignment is creating an agent that uses Monte-Carlo Tree Search (MCTS) that can take a game to play it.\n",
        "\n",
        "##**A Quick Introduction to MCTS**\n",
        "Monte Carlo Tree Search (MCTS) is a popular algorithm for making decisions in large, complex environments, especially games like Go, Chess, and Ultimate Tic-Tac-Toe.\n",
        "\n",
        "At its core, MCTS builds a search tree dynamically by simulating many possible future plays of the game. Instead of analyzing all moves exhaustively (which would be impossible in very large games), it randomly explores promising options and gradually focuses on the best strategies.\n",
        "\n",
        "**MCTS operates through four key phases:**\n",
        "\n",
        "- **Selection**\n",
        "  - Starting at the root, recursively select child nodes that appear most promising, balancing exploration (trying new moves) and exploitation (choosing moves known to be good).\n",
        "- **Expansion**\n",
        "  - When reaching a node with unexplored moves, add one or more child nodes representing these new possibilities.\n",
        "- **Simulation (Rollout)**\n",
        "  - From the newly expanded node, simulate a random (or semi-random) playout to the end of the game to estimate how good that move might be.\n",
        "- **Backpropagation**\n",
        "  - Propagate the result of the simulation back up the tree, updating statistics (like win counts) for all nodes visited during selection.\n",
        "\n",
        "By repeating these four phases a number of times, MCTS gradually builds a \"map\" of the most promising moves without ever having to fully solve the game.\n",
        "\n",
        "##Additional Resources\n",
        "- [Wikipedia](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)\n",
        "- [Builtin](https://builtin.com/machine-learning/monte-carlo-tree-search)\n",
        "- [Video](https://www.youtube.com/watch?v=UXW2yZndl7U) by John Levine\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "74AVR7-ube5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 2: Implementation**\n",
        "\n",
        "For this, we will be splitting it into 4 small componnets:\n",
        "\n",
        "- Component 1: MCTS Nodes\n",
        "\n",
        "- Component 2: State Copy Function\n",
        "\n",
        "- Component 3: MCTS Main Loop\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "S7zz1QNzrzSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Component 1: MCTS Nodes**\n",
        "\n",
        "The first of these components is the Node itself.\n",
        "\n",
        "Each node should:\n",
        "\n",
        "- Store the state of the game at that point.\n",
        "\n",
        "- Keep track of which move led to that node from its parent.\n",
        "\n",
        "- Record statistics like how many times it has been visited, and what the total reward is.\n",
        "\n",
        "- Track which moves are still untried from that position.\n",
        "\n",
        "The node should allow you to:\n",
        "\n",
        "- Check if it’s fully expanded (i.e., all moves have been tried).\n",
        "\n",
        "- Select one of its children to continue searching, balancing exploitation (good moves) and exploration (trying unknown moves).\n",
        "\n",
        "You are encouraged to use a well-known formula like UCB1 for this purpose."
      ],
      "metadata": {
        "id": "GZkT5SFCWAQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write Your Code Here"
      ],
      "metadata": {
        "id": "vtyzf3ZQ1Twr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Component 2: State Copying**\n",
        "\n",
        "Since MCTS simulates future moves many times, it’s important that changing the board in one simulation does not affect the real game.\n",
        "\n",
        "The second important component here is a function that makes copyies of the Ultimate-Tic-Tac-Toe Board\n",
        "\n",
        "Think about how to create an independent copy of:\n",
        "\n",
        "- The board's current layout,\n",
        "\n",
        "- Which player’s turn it is,\n",
        "\n",
        "- Any extra game metadata (e.g., where the next move must be played)."
      ],
      "metadata": {
        "id": "ZFAj2uAfztyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write Your Code Here"
      ],
      "metadata": {
        "id": "RJqbvep114qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Component 3: MCTS Main Loop**\n",
        "\n",
        "The Final component is the Main Loop/function itself. This is where we tie together all the components and complete the expansion and selection side of things.\n",
        "\n",
        "##This includes:\n",
        "\n",
        "Expanding New Moves\n",
        "- When reaching a node where there are still moves that haven't been tried:\n",
        "  - Pick one of the untried moves,\n",
        "  - Create a new node representing the resulting game state after playing that move,\n",
        "  - Add it to the tree.\n",
        "\n",
        "\n",
        "Simulating a Random Game (Rollout)\n",
        "- From the newly created node:\n",
        "  - Play random moves until the game ends (win, lose, or draw).\n",
        "  - This simulation does not have to be smart — just choose valid random moves at each step.\n",
        "\n",
        "Backpropagating Results\n",
        "- After the simulation finishes:\n",
        "  - Update statistics for all nodes along the path you traversed (from root to leaf).\n",
        "  - Each node should record:\n",
        "    - One more visit\n",
        "    - An updated total reward depending on the result.\n",
        "\n",
        "- When updating rewards:\n",
        "  - Reward should be viewed from the perspective of the root player,  not from the perspective of the node’s current player.\n",
        "\n",
        "Choosing the Final Move\n",
        "- After running many simulations:\n",
        "  - Look at all the children of the root node,\n",
        "  - Choose the child that was visited the most during search,\n",
        "  - Return the move that led to that child.\n"
      ],
      "metadata": {
        "id": "ma2mwkTSzxkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write Your Code Here"
      ],
      "metadata": {
        "id": "coijA93c18Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Testing Your MCTS Bot** (X Points)\n",
        "\n",
        "Have your MCTS Bot play against both the random agent for 5 games, and have it play against itself for 5 games.\n",
        "\n",
        "Modify its iteration count for each of the games, and then do the following for each game:\n",
        "\n",
        "- Print out a table containing the Iteration count and its corresponding winrate\n",
        "\n",
        "- Perform small write up on the results, and if they matched your predictions\n"
      ],
      "metadata": {
        "id": "EWYy9qcvgMO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write Your Code Here"
      ],
      "metadata": {
        "id": "PicxfjC6G79R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: Improving the Bot** (X Points)\n",
        "\n",
        "Now that we have a basic implementation of MCTS, Lets improve it.\n",
        "\n",
        "Lets look at Improving the rollout/simulation function and the Reward System for our bot.\n",
        "\n",
        "Write an Enhanced Version of your MCTS Bot that implements the following to your Rollout Function and Reward System.\n",
        "\n",
        "- A Smarter Rollout/Simulation Function (Non-random simulation heuristics)\n",
        "\n",
        "- Develop better rewards:\n",
        "\n",
        "- Improving the Node Selection Formula:\n",
        "\n",
        "If you would like some help or additional reading to understand this better:\n",
        "\n",
        "- [Reading 1 (Reccomended)](https://www.uttt.ai/blog)\n",
        "- [Reading 2 (Complicated)](https://suragnair.github.io/posts/alphazero.html)\n",
        "- [Reading 3 (LONG)](https://philhchen.github.io/files/tictactoe-paper.pdf)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ErlsBZenfAny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Improvement 1: Nodes!**\n",
        "\n",
        "The first of these Improvements is to the MCTS Node.\n",
        "\n",
        "In the basic MCTS version, all legal moves were treated equally during expansion and selection. But not all moves are equal.\n",
        "\n",
        "Some positions (like the center of a board) are much stronger than others — and experienced players (and bots!) often prioritize these.\n",
        "\n",
        "You should modify how your nodes prioritize which moves to try. Specifically:\n",
        "\n",
        "- Assign a “prior score” or weight to each move, based on how good it seems before simulating it.\n",
        "\n",
        "- Use this prior to bias the move selection during tree search.\n",
        "\n",
        "Here are some simple heuristics you can use to score moves:\n",
        "\n",
        "- Moves to the center of a small board are better.\n",
        "\n",
        "- Corner positions are better than sides.\n",
        "\n",
        "- If a move wins a small board, it's much better.\n",
        "\n",
        "- Moves that send the opponent to a finished board (giving you more freedom) are better.\n",
        "\n",
        "Another Improvement is the formula used!\n",
        "\n",
        "- If you used UCB1, you should improve the formula to the PUCT\n",
        "\n",
        "(Read [this](https://towardsdatascience.com/playing-chess-with-a-generalized-ai-b83d64ac71fe/) to get some insight)"
      ],
      "metadata": {
        "id": "JNm3NRCNKRTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write Your Code Here"
      ],
      "metadata": {
        "id": "zCPES9KSKR1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Improvement 2: Reward Systems!**\n",
        "\n",
        "Your basic agent treated outcomes as either a win (1), tie (0.5), or loss (0).\n",
        "This is very coarse and only tells the agent if it won at the end — it doesn’t help during the rest of the game.\n",
        "\n",
        "Instead, you should define a reward function that can give credit for partial progress — even if the game hasn’t ended yet.\n",
        "\n",
        "Improve/Create the system so that it rewards:\n",
        "\n",
        "- Rewards winning a small board (e.g., +0.1),\n",
        "\n",
        "- Penalizes losing a small board (e.g., -0.1),\n",
        "\n",
        "- Gives extra reward for controlling multiple boards in a row, or the center board,\n",
        "\n",
        "- Still gives a larger reward for winning the full game (e.g., +1.0), and a smaller reward for drawing (e.g., 0.4),\n",
        "\n",
        "- Optionally uses a squashing function (like tanh) to keep values in a reasonable range.\n",
        "\n",
        "This lets your MCTS agent backpropagate better signals — not just \"Did I win?\" but rather \"How good is this board overall?\""
      ],
      "metadata": {
        "id": "3wEEx69pLa1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write Your Code Here"
      ],
      "metadata": {
        "id": "EvNCmfGOLbPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Improvement 3: Main Loop (Propogation/Simulation)**\n",
        "\n",
        "In your original MCTS agent, simulations were completely random.\n",
        "This works okay for simple games, but in Ultimate Tic-Tac-Toe, random play often leads to noisy, misleading results.\n",
        "\n",
        "Your goal is to make the simulation (or \"rollout\") smarter, so it gives better estimates of move quality.\n",
        "\n",
        "Try to update your simulation logic so that it uses a heuristic.\n",
        "\n",
        "Ex.\n",
        " - Prefer moves that complete small boards.\n",
        " - Prioritize moves that block the opponent from winning small boards.\n",
        " - Avoid bad or useless moves (e.g., playing moves that results in letting the opponent play in any board).\n",
        "\n",
        "You don’t need to build a full search or evaluation algorithm inside the simulation. But even a few simple heuristics can make the playouts way more realistic."
      ],
      "metadata": {
        "id": "0q37kbUAVvso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write Your Code Here"
      ],
      "metadata": {
        "id": "6I_UmCdEfs7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 4: The Gauntlet** (X Points)\n",
        "\n",
        "Lets do a set of 1v1 tests between your MCTS Agent and the less than perfect ones below.\n",
        "\n",
        "The following are 3 additional Agents, each with a special quirk.\n",
        "\n",
        "###For each agent:\n",
        "- Complete the Predictions\n",
        "\n",
        "- Have your Monte-Carlos Agent Play against the challenge Agent, at 4 different iteration counts. (5 game for each is good enough)\n",
        "\n",
        "- Print out a table containing the Iteration count and its corresponding winrate\n",
        "\n",
        "- Use the Visualizer to create a final board summary.\n",
        "\n",
        "- Perform small write up on the results, and if they matched your predictions\n",
        "\n",
        "Hint: You may be able to reuse your previous test code for Part 2's Testing here!"
      ],
      "metadata": {
        "id": "pwRT5EgUfBHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Round 1: Improved vs Sir Abserd\n",
        "\n",
        "Sir Abserd is as random as random comes, and makes moves with no rhyme or reason.\n",
        "\n",
        "Abserd:\n",
        " - Makes Random Moves"
      ],
      "metadata": {
        "id": "M3J5nCoOlnvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_ser_abserd(board):\n",
        "    return random.choice(board.get_legal_moves())"
      ],
      "metadata": {
        "id": "bkDy3Xgxfqpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction:\n",
        "- Give a Range, how many moves do you think it'll take to win?\n",
        "- At what Iteration Count do you think it'll take your bot to reach above 50% win rate."
      ],
      "metadata": {
        "id": "45fPNexmBb_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write Your Code Here"
      ],
      "metadata": {
        "id": "Y0GChOZeBbgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Round 2: Monte-Carlo vs Near Seer**\n",
        "Near Seer can see the future, so long as it's within a 3x3 block.\n",
        "\n",
        "Near Seer:\n",
        "  - Tries to make a win on a small board if it sees one"
      ],
      "metadata": {
        "id": "cP0wYRfhlqNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_near_seer(board):\n",
        "    moves = board.get_legal_moves()\n",
        "    for move in moves:\n",
        "        big_r, big_c, small_r, small_c = move\n",
        "        small_board = board.boards[big_r][big_c]\n",
        "        temp_grid = small_board.grid.copy()\n",
        "        temp_grid[small_r, small_c] = board.current_player\n",
        "\n",
        "        # Check for immediate win in small board\n",
        "        for i in range(3):\n",
        "            if all(temp_grid[i, :] == board.current_player) or all(temp_grid[:, i] == board.current_player):\n",
        "                return move\n",
        "        if all(np.diag(temp_grid) == board.current_player) or all(np.diag(np.fliplr(temp_grid)) == board.current_player):\n",
        "            return move\n",
        "    return random.choice(moves)"
      ],
      "metadata": {
        "id": "aD1dUUl8lnY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction:\n",
        "- Give a Range, how many moves do you think it'll take to win?\n",
        "- At what Iteration Count do you think it'll take your bot to reach above 50% win rate."
      ],
      "metadata": {
        "id": "QyNBQu2EXrvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write Your Code Here"
      ],
      "metadata": {
        "id": "vuXG7XFdXq_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Round 3: Monte-Carlo vs Count Erfit**\n",
        "\n",
        "Count Erfit tries to mirror his last move, and if he can't, he'll mirror your last move.\n",
        "\n",
        "Count Erfit:\n",
        "  - First, tries to play same (small_r, small_c) as last own move\n",
        "  - If not available, tries to play the exact opponent last move\n",
        "  - If not, plays random\n"
      ],
      "metadata": {
        "id": "ph1nDUsPlp-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_erfit(board, last_self_move, last_opponent_move):\n",
        "    moves = board.get_legal_moves()\n",
        "\n",
        "    if last_self_move:\n",
        "        _, _, last_small_r, last_small_c = last_self_move\n",
        "        for move in moves:\n",
        "            if move[2] == last_small_r and move[3] == last_small_c:\n",
        "                return move\n",
        "\n",
        "    if last_opponent_move:\n",
        "        target_move = last_opponent_move\n",
        "        if target_move in moves:\n",
        "            return target_move\n",
        "\n",
        "    # Otherwise random\n",
        "    return random.choice(moves)\n",
        "#Wrapper\n",
        "def agent_count_erfit():\n",
        "    last_self_move = [None]\n",
        "    last_opponent_move = [None]\n",
        "\n",
        "    def agent(game):\n",
        "        move = count_erfit(game, last_self_move[0], last_opponent_move[0])\n",
        "        last_opponent_move[0] = move\n",
        "        last_self_move[0] = move\n",
        "        return move\n",
        "\n",
        "    return agent"
      ],
      "metadata": {
        "id": "4Rf25SUSlnMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction:\n",
        "- Give a Range, how many moves do you think it'll take to win?\n",
        "- At what Iteration Count do you think it'll take your bot to reach above 50% win rate."
      ],
      "metadata": {
        "id": "DKf6I722FDnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write Your Code Here"
      ],
      "metadata": {
        "id": "gf49UqHnFFG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Final Test: Improved Monte-Carlo Vs Unimproved Monte-Carlo**\n",
        "\n",
        "At the end of the Gaunlet, the greatest battle is against one-self.\n",
        "\n",
        "For this test, have the your Improved Monte Carlo Implementation compete vs the basic version from Part 2.\n",
        "\n",
        "This agent should match the improved version's Iteration Count\n"
      ],
      "metadata": {
        "id": "gt17PkN_ECf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction:\n",
        "- Give a Range, how many moves do you think it'll take to win?\n",
        "- At what Iteration Count do you think it'll take your bot to reach above 50% win rate."
      ],
      "metadata": {
        "id": "v9_pAquaFGMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write Your Code Here"
      ],
      "metadata": {
        "id": "uGBN1KP5EDEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Discussion!\n",
        "\n",
        "Think about the 4 tests we have done with the Improved Monte-Carlo Implementation.\n",
        "\n",
        "Can the Monte-Carlo bot be improved any further? If so, how? If not, why not?"
      ],
      "metadata": {
        "id": "GpyqNQ0HluJf"
      }
    }
  ]
}