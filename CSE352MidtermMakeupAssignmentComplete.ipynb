{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Czedros/CSE352-Machine-Learning-Assignments/blob/main/CSE352MidtermMakeupAssignmentComplete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Introduction**\n",
        "\n",
        "Name: [Enter Here]\n",
        "\n",
        "I understand that my submission needs to be my own work: [Initials]\n",
        "\n",
        "I understand that ChatGPT / Copilot / other AI tools are not allowed:  [Initials]\n",
        "\n",
        "Total Points: **X**\n",
        "\n",
        "Complete this notebook and submit it (save/print as pdf). Make sure all output is correct in the pdf before submitting (it sometimes gets cut off).\n",
        "\n",
        " The notebook needs to be a complete project report with your implementation, documentation including a short discussion of how your implementation works and your design choices, and experimental results (e.g., tables and charts with simulation results) with a short discussion of what they mean. Use the provided notebook cells and insert additional code and markdown cells as needed."
      ],
      "metadata": {
        "id": "or-AfmlaFBvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview**\n",
        "\n",
        "In this assignment, you will:\n",
        "- Implement the rules and structure for Ultimate Tic-Tac-Toe\n",
        "- Design a Monte Carlo Tree Search (MCTS) agent\n",
        "- Improve your Monto Carlo Tree Search Agent\n",
        "- Defeat a Cohort of Challengers with your agent.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jlbGPJMxmWXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ultimate Tic-Tac-Toe**\n",
        "\n",
        "Ultimate Tic-Tac-Toe is a strategic extension of the classic Tic-Tac-Toe game. Rather than a single 3×3 grid, Ultimate Tic-Tac-Toe consists of a 3×3 grid where each cell contains another 3×3 Tic-Tac-Toe board. This layered setup creates a more complex game structure requiring players to think multiple moves ahead.\n",
        "\n",
        "The game was first popularized online in the early 2010s and is designed to emphasize positional play, long-term strategy, and adaptation to an evolving board state. Each player, X or O, takes turns placing their mark in a cell of one of the smaller boards. However, the twist lies in the move constraints: a player’s move determines which of the nine small boards their opponent must play in next. Specifically, if a player places their mark in cell (i, j) of a small board, their opponent must play in the (i, j) small board on their next turn.\n",
        "\n",
        "The objective is to win the larger game by winning three small boards in a row—horizontally, vertically, or diagonally—similar to classic Tic-Tac-Toe. A small board is won when a player has three marks in a row within it. If a player is sent to a board that is already won or drawn, they may play in any open board of their choosing.\n",
        "\n",
        "Ultimate Tic-Tac-Toe presents a more intricate challenge than its predecessor and is a rich environment for studying game trees, simulation-based planning, and reinforcement learning.\n",
        "\n",
        "Additional Reading:\n",
        "\n",
        "[Wikipedia page](https://en.wikipedia.org/wiki/Ultimate_tic-tac-toe).\n",
        "\n",
        "\n",
        "[Quick Video Demonstration on how the Game works](https://www.youtube.com/shorts/_Na3a1ZrX7c) (Credit: VSauce)\n",
        "\n",
        "[More In-Depth Guide](https://www.youtube.com/watch?v=37PC0bGMiTI) (Credit: MindYourDecisions - Presh Talwalkar)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dYcSRE2jmWMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualizer Helper\n",
        "\n",
        "There are 2 ways to go about creating a board for Ultimate Tic Tac Toe, a 9x9 grid or a 3x3 of 3x3 grids. These will result in different ways to visualize the board.\n",
        "\n",
        "\n",
        "The following is a visuzlizing tool and 2 adapters that can be used depending on your implementation of the game.\n",
        "\n",
        "#Implementation 1: 9x9 Boards"
      ],
      "metadata": {
        "id": "dvY0Xy_SO54c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def draw_ultimate_board(board, small_board_winners=None, last_move=None, highlight=None):\n",
        "    \"\"\"\n",
        "    Visualizes a 9x9 Ultimate Tic-Tac-Toe board:\n",
        "    - Grays out small boards that have been won\n",
        "    - Draws a large X or O over won small boards\n",
        "    - Highlights last move in bold\n",
        "    - Red border for active small board\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(6,6))\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xlim(0, 9)\n",
        "    ax.set_ylim(0, 9)\n",
        "\n",
        "    # Draw grid\n",
        "    for i in range(10):\n",
        "        lw = 2 if i % 3 == 0 else 0.5\n",
        "        ax.axhline(i, color='black', lw=lw, zorder=0)\n",
        "        ax.axvline(i, color='black', lw=lw, zorder=0)\n",
        "\n",
        "    # Draw faded backgrounds for won small boards\n",
        "    if small_board_winners:\n",
        "        for br in range(3):\n",
        "            for bc in range(3):\n",
        "                winner = small_board_winners[br][bc]\n",
        "                if winner in [1, 2]:\n",
        "                    rect = plt.Rectangle((bc*3, 6 - br*3), 3, 3, color='gray', alpha=0.6, zorder=1)\n",
        "                    ax.add_patch(rect)\n",
        "                    ax.text(bc*3 + 1.5, 7.5 - br*3, 'X' if winner == 1 else 'O',\n",
        "                            fontsize=80, ha='center', va='center', color='black', alpha=1, zorder=2)\n",
        "\n",
        "    # Draw pieces\n",
        "    for row in range(9):\n",
        "        for col in range(9):\n",
        "            val = board[row, col]\n",
        "            if val == 1:\n",
        "                ax.text(col+0.5, 8.5 - row, 'X', ha='center', va='center', fontsize=16,\n",
        "                        fontweight='bold' if (last_move == (row, col)) else 'normal', zorder=3)\n",
        "            elif val == 2:\n",
        "                ax.text(col+0.5, 8.5 - row, 'O', ha='center', va='center', fontsize=16,\n",
        "                        fontweight='bold' if (last_move == (row, col)) else 'normal', zorder=3)\n",
        "\n",
        "    # Highlight active small board\n",
        "    if highlight:\n",
        "        hrow, hcol = highlight\n",
        "        x0 = hcol * 3\n",
        "        y0 = 6 - hrow * 3\n",
        "        ax.plot([x0, x0+3], [y0, y0], color='red', lw=4, zorder=4)\n",
        "        ax.plot([x0, x0+3], [y0+3, y0+3], color='red', lw=4, zorder=4)\n",
        "        ax.plot([x0, x0], [y0, y0+3], color='red', lw=4, zorder=4)\n",
        "        ax.plot([x0+3, x0+3], [y0, y0+3], color='red', lw=4, zorder=4)\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "1jK3QFHrFBbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##An example of the visualizer code and how its used"
      ],
      "metadata": {
        "id": "JQPSrO8uRTTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_board = np.zeros((9, 9), dtype=int)\n",
        "test_board[0, 0] = 1\n",
        "test_board[1, 1] = 1\n",
        "test_board[2, 2] = 1\n",
        "test_board[3, 3] = 2\n",
        "test_board[4, 4] = 2\n",
        "test_board[5, 5] = 2\n",
        "\n",
        "small_board_winners = [\n",
        "    [1, 0, 0],\n",
        "    [0, 2, 0],\n",
        "    [0, 0, 0]\n",
        "]\n",
        "\n",
        "last_move = (5, 5)\n",
        "highlight = (1, 1)\n",
        "\n",
        "draw_ultimate_board(test_board, small_board_winners, last_move, highlight)"
      ],
      "metadata": {
        "id": "k96Fi8lZRSlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adapter 1: Using 3x3 of 3x3 arrays"
      ],
      "metadata": {
        "id": "PMJkNCWR_2_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nested_list_to_board(nested_game, highlight=None, last_move=None, winners=None):\n",
        "    board_array = np.zeros((9, 9), dtype=int)\n",
        "    for br in range(3):\n",
        "        for bc in range(3):\n",
        "            sb = nested_game[br][bc]\n",
        "            if isinstance(sb, np.ndarray):\n",
        "                board_array[br*3:(br+1)*3, bc*3:(bc+1)*3] = sb\n",
        "            else:\n",
        "                board_array[br*3:(br+1)*3, bc*3:(bc+1)*3] = np.array(sb)\n",
        "\n",
        "    draw_ultimate_board(board_array, small_board_winners=winners, last_move=last_move, highlight=highlight)"
      ],
      "metadata": {
        "id": "5xbUi3CzsPBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example Usage"
      ],
      "metadata": {
        "id": "Jem1Y1UA_6Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nested_game = [\n",
        "    [np.array([[1,0,0],[0,1,0],[0,0,1]]), np.zeros((3,3)), np.zeros((3,3))],\n",
        "    [np.zeros((3,3)), np.array([[2,0,0],[0,2,0],[0,0,2]]), np.zeros((3,3))],\n",
        "    [np.zeros((3,3)), np.zeros((3,3)), np.zeros((3,3))]\n",
        "]\n",
        "\n",
        "winners = [\n",
        "    [1, 0, 0],\n",
        "    [0, 2, 0],\n",
        "    [0, 0, 0]\n",
        "]\n",
        "\n",
        "nested_list_to_board(nested_game, highlight=(0,0), last_move=(0,0), winners=winners)\n"
      ],
      "metadata": {
        "id": "veMyisXgrkps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1: Ultimate Tic-Tac-Toe [X Points]\n",
        "In the first part, you will implement the mechanics of Ultimate Tic-Tac-Toe, defining how the game progresses and determining legal moves and win conditions.\n",
        "\n",
        "There are an infinite number of ways to do this, and the guidelines will be pretty Lax\n",
        "\n",
        "Here are two reccomended Methods and the Specifications\n",
        "\n",
        "##Method 1: 3x3 Implementation\n",
        "\n",
        "- Create a Small Board Class that contains a basic Tic-Tac-Toe Game\n",
        "- Create a Big Board Class that contains a 3x3 grid of Small boards\n",
        "- Implement logic for Tic tac toe..\n",
        "\n",
        "##Method 2: Loose 9x9 Implementation\n",
        "- Make a 9x9 grid\n",
        "- Whenever a Move is made, Check for legality of the move and make it\n",
        "- Check for wins via checking every 3x3 in code.\n",
        "\n",
        "##Requirements\n",
        "\n",
        "Regardless of the methods, several things are mandatory.\n",
        "\n",
        "- get_legal_moves(self) : Gives a list of legal plays (This is important for the bots!)\n",
        "\n",
        "- use big_r, big_c, small_r, small_c as the format of the moves (This is important for the bots!)\n",
        "\n",
        "\n",
        "(Alternatively, give this to students as is, and have them complete just the bots,those are alot of work already, and this was pretty simple)"
      ],
      "metadata": {
        "id": "PFkDGV_BObd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class SmallBoard:\n",
        "    def __init__(self):\n",
        "        self.grid = np.zeros((3, 3), dtype=int)  # 0 = empty, 1 = X, 2 = O\n",
        "        self.winner = 0\n",
        "        self.active = True\n",
        "\n",
        "    def play_move(self, row, col, player):\n",
        "        if self.grid[row, col] != 0 or not self.active:\n",
        "            return False\n",
        "        self.grid[row, col] = player\n",
        "        self.check_winner()\n",
        "        return True\n",
        "\n",
        "    def check_winner(self):\n",
        "        for i in range(3):\n",
        "            if np.all(self.grid[i, :] == self.grid[i, 0]) and self.grid[i, 0] != 0:\n",
        "                self._declare_winner(self.grid[i, 0])\n",
        "                return\n",
        "            if np.all(self.grid[:, i] == self.grid[0, i]) and self.grid[0, i] != 0:\n",
        "                self._declare_winner(self.grid[0, i])\n",
        "                return\n",
        "        if np.all(np.diag(self.grid) == self.grid[0, 0]) and self.grid[0, 0] != 0:\n",
        "            self._declare_winner(self.grid[0, 0])\n",
        "            return\n",
        "        if np.all(np.diag(np.fliplr(self.grid)) == self.grid[0, 2]) and self.grid[0, 2] != 0:\n",
        "            self._declare_winner(self.grid[0, 2])\n",
        "            return\n",
        "        if np.all(self.grid != 0):\n",
        "            self._declare_winner(-1)\n",
        "\n",
        "    def _declare_winner(self, winner):\n",
        "        self.winner = winner\n",
        "        self.active = False\n"
      ],
      "metadata": {
        "id": "BOB-BnkwO4vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UltimateBoard:\n",
        "    def __init__(self):\n",
        "        self.boards = [[SmallBoard() for _ in range(3)] for _ in range(3)]\n",
        "        self.current_player = 1  # 1 = X, 2 = O\n",
        "        self.next_board = None\n",
        "        self.global_winner = 0\n",
        "        self.last_move = None\n",
        "\n",
        "    def play_move(self, big_r, big_c, small_r, small_c):\n",
        "        board = self.boards[big_r][big_c]\n",
        "        if self.next_board and (big_r, big_c) != self.next_board:\n",
        "            print(\"You must play in board\", self.next_board)\n",
        "            return False\n",
        "        if not board.play_move(small_r, small_c, self.current_player):\n",
        "            print(\"Invalid move.\")\n",
        "            return False\n",
        "\n",
        "        next_r, next_c = small_r, small_c\n",
        "        if self.boards[next_r][next_c].active:\n",
        "            self.next_board = (next_r, next_c)\n",
        "        else:\n",
        "            self.next_board = None\n",
        "\n",
        "        self.check_global_winner()\n",
        "        self.current_player = 3 - self.current_player\n",
        "        self.last_move = (big_r, big_c, small_r, small_c)\n",
        "        return True\n",
        "\n",
        "    def check_global_winner(self):\n",
        "        grid = np.array([[b.winner if b.winner > 0 else 0 for b in row] for row in self.boards])\n",
        "        for i in range(3):\n",
        "            if np.all(grid[i, :] == grid[i, 0]) and grid[i, 0] != 0:\n",
        "                self.global_winner = grid[i, 0]\n",
        "                return\n",
        "            if np.all(grid[:, i] == grid[0, i]) and grid[0, i] != 0:\n",
        "                self.global_winner = grid[0, i]\n",
        "                return\n",
        "        if np.all(np.diag(grid) == grid[0, 0]) and grid[0, 0] != 0:\n",
        "            self.global_winner = grid[0, 0]\n",
        "            return\n",
        "        if np.all(np.diag(np.fliplr(grid)) == grid[0, 2]) and grid[0, 2] != 0:\n",
        "            self.global_winner = grid[0, 2]\n",
        "            return\n",
        "\n",
        "    def get_legal_moves(self):\n",
        "        moves = []\n",
        "        for big_r in range(3):\n",
        "            for big_c in range(3):\n",
        "                board = self.boards[big_r][big_c]\n",
        "                if self.next_board and (big_r, big_c) != self.next_board:\n",
        "                    continue\n",
        "                if board.active:\n",
        "                    for i in range(3):\n",
        "                        for j in range(3):\n",
        "                            if board.grid[i, j] == 0:\n",
        "                                moves.append((big_r, big_c, i, j))\n",
        "        return moves\n",
        "\n",
        "    def display(self):\n",
        "        def cell_symbol(val):\n",
        "            return '.' if val == 0 else ('X' if val == 1 else 'O')\n",
        "        for big_r in range(3):\n",
        "            for r in range(3):\n",
        "                line = ''\n",
        "                for big_c in range(3):\n",
        "                    board = self.boards[big_r][big_c]\n",
        "                    line += ' '.join(cell_symbol(board.grid[r, c]) for c in range(3)) + ' | '\n",
        "                print(line)\n",
        "            print('-' * 20)\n"
      ],
      "metadata": {
        "id": "fW2E4zkAW9Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ultimate_board_to_board(game, highlight=None):\n",
        "    board_array = np.zeros((9, 9), dtype=int)\n",
        "    small_board_winners = [[game.boards[r][c].winner for c in range(3)] for r in range(3)]\n",
        "    last_move = None\n",
        "    if hasattr(game, 'last_move') and game.last_move:\n",
        "        br, bc, sr, sc = game.last_move\n",
        "        last_move = (br*3 + sr, bc*3 + sc)\n",
        "\n",
        "    for br in range(3):\n",
        "        for bc in range(3):\n",
        "            sb = game.boards[br][bc].grid\n",
        "            board_array[br*3:(br+1)*3, bc*3:(bc+1)*3] = sb\n",
        "\n",
        "    if highlight is None and hasattr(game, 'next_board') and not game.global_winner:\n",
        "        highlight = game.next_board\n",
        "\n",
        "    draw_ultimate_board(board_array, small_board_winners=small_board_winners,\n",
        "                        last_move=last_move, highlight=highlight)\n"
      ],
      "metadata": {
        "id": "dIZLcKwpsZrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_game(agent_X=None, agent_O=None):\n",
        "    game = UltimateBoard()\n",
        "\n",
        "    while not game.global_winner and game.get_legal_moves():\n",
        "        print(\"\\nCurrent Turn: Player {}\".format(\"X\" if game.current_player == 1 else \"O\"))\n",
        "        ultimate_board_to_board(game)\n",
        "\n",
        "        if game.next_board:\n",
        "            print(\"You must play in small board:\", game.next_board)\n",
        "        else:\n",
        "            print(\"You may play in any board.\")\n",
        "\n",
        "        # Determine agent for this turn\n",
        "        agent = agent_X if game.current_player == 1 else agent_O\n",
        "\n",
        "        if agent is None:\n",
        "            # Human move\n",
        "            move = input(\"Enter your move (big_r big_c small_r small_c): \").split()\n",
        "            try:\n",
        "                move = tuple(map(int, move))\n",
        "                if len(move) != 4 or not game.play_move(*move):\n",
        "                    print(\"Try again.\")\n",
        "            except:\n",
        "                print(\"Invalid input.\")\n",
        "        else:\n",
        "            # Bot move\n",
        "            move = agent(game)\n",
        "            print(f\"Bot plays: {move}\")\n",
        "            game.play_move(*move)\n",
        "\n",
        "    ultimate_board_to_board(game)\n",
        "    if game.global_winner:\n",
        "        print(\"Winner: Player\", \"X\" if game.global_winner == 1 else \"O\")\n",
        "    else:\n",
        "        print(\"Game ended in a draw.\")"
      ],
      "metadata": {
        "id": "1nUoQUgkW9lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test 1: Random Bot\n",
        "\n",
        "Implement a bot that makes Random Moves, and have it play against itself! Use this to test out whether your game is working correctly, and see how the game plays.\n"
      ],
      "metadata": {
        "id": "q0SJ1cfEXVZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_bot(board: UltimateBoard):\n",
        "    return random.choice(board.get_legal_moves())\n",
        "\n",
        "def play_botvbot_game(agent_X=None, agent_O=None):\n",
        "    game = UltimateBoard()\n",
        "\n",
        "    while not game.global_winner and game.get_legal_moves():\n",
        "        agent = agent_X if game.current_player == 1 else agent_O\n",
        "\n",
        "        if agent is None:\n",
        "            move = input(\"Enter your move (big_r big_c small_r small_c): \").split()\n",
        "            try:\n",
        "                move = tuple(map(int, move))\n",
        "                if len(move) != 4 or not game.play_move(*move):\n",
        "                    print(\"Try again.\")\n",
        "            except:\n",
        "                print(\"Invalid input.\")\n",
        "        else:\n",
        "            move = agent(game)\n",
        "            game.play_move(*move)\n",
        "\n",
        "    print(\"\\nFinal Result:\")\n",
        "    ultimate_board_to_board(game)\n",
        "    if game.global_winner:\n",
        "        print(\"Winner: Player\", \"X\" if game.global_winner == 1 else \"O\")\n",
        "    else:\n",
        "        print(\"Game ended in a draw.\")\n",
        "\n",
        "    return game.global_winner\n",
        "\n",
        "play_botvbot_game(agent_X=random_bot, agent_O=random_bot)"
      ],
      "metadata": {
        "id": "WZYW0hW-wHC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 2: Monte Carlo Tree Search [X Points]**\n",
        "\n",
        "The second part of this assignment is creating an agent that uses Monte-Carlo Tree Search (MCTS) that can take a game to play it.\n",
        "\n",
        "##**A Quick Introduction to MCTS**\n",
        "Monte Carlo Tree Search (MCTS) is a popular algorithm for making decisions in large, complex environments, especially games like Go, Chess, and Ultimate Tic-Tac-Toe.\n",
        "\n",
        "At its core, MCTS builds a search tree dynamically by simulating many possible future plays of the game. Instead of analyzing all moves exhaustively (which would be impossible in very large games), it randomly explores promising options and gradually focuses on the best strategies.\n",
        "\n",
        "**MCTS operates through four key phases:**\n",
        "\n",
        "- **Selection**\n",
        "  - Starting at the root, recursively select child nodes that appear most promising, balancing exploration (trying new moves) and exploitation (choosing moves known to be good).\n",
        "- **Expansion**\n",
        "  - When reaching a node with unexplored moves, add one or more child nodes representing these new possibilities.\n",
        "- **Simulation (Rollout)**\n",
        "  - From the newly expanded node, simulate a random (or semi-random) playout to the end of the game to estimate how good that move might be.\n",
        "- **Backpropagation**\n",
        "  - Propagate the result of the simulation back up the tree, updating statistics (like win counts) for all nodes visited during selection.\n",
        "\n",
        "By repeating these four phases a number of times, MCTS gradually builds a \"map\" of the most promising moves without ever having to fully solve the game.\n",
        "\n",
        "##Additional Resources\n",
        "- [Wikipedia](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)\n",
        "- [Builtin](https://builtin.com/machine-learning/monte-carlo-tree-search)\n",
        "- [Video](https://www.youtube.com/watch?v=UXW2yZndl7U) by John Levine\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "74AVR7-ube5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 2: Implementation**\n",
        "\n",
        "For this, we will be splitting it into 4 small componnets:\n",
        "\n",
        "- Component 1: MCTS Nodes\n",
        "\n",
        "- Component 2: State Copy Function\n",
        "\n",
        "- Component 3: MCTS Main Loop\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "S7zz1QNzrzSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Component 1: MCTS Nodes**\n",
        "\n",
        "The first of these components is the Node itself.\n",
        "\n",
        "Each node should:\n",
        "\n",
        "- Store the state of the game at that point.\n",
        "\n",
        "- Keep track of which move led to that node from its parent.\n",
        "\n",
        "- Record statistics like how many times it has been visited, and what the total reward is.\n",
        "\n",
        "- Track which moves are still untried from that position.\n",
        "\n",
        "The node should allow you to:\n",
        "\n",
        "- Check if it’s fully expanded (i.e., all moves have been tried).\n",
        "\n",
        "- Select one of its children to continue searching, balancing exploitation (good moves) and exploration (trying unknown moves).\n",
        "\n",
        "You are encouraged to use a well-known formula like UCB1 for this purpose."
      ],
      "metadata": {
        "id": "GZkT5SFCWAQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "class MCTSNode:\n",
        "    def __init__(self, game_state, parent=None, move=None):\n",
        "        self.game_state = game_state\n",
        "        self.parent = parent\n",
        "        self.move = move\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.total_reward = 0.0\n",
        "        self.untried_moves = game_state.get_legal_moves() if game_state.global_winner == 0 else []\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        return len(self.untried_moves) == 0\n",
        "\n",
        "    def select_child(self, exploration_weight=1.414):\n",
        "        best_score = -float('inf')\n",
        "        best_children = []\n",
        "\n",
        "        for child in self.children:\n",
        "            if child.visits == 0:\n",
        "                score = float('inf')\n",
        "            else:\n",
        "                # UCB1 formula\n",
        "                exploitation = child.total_reward / child.visits\n",
        "                exploration = exploration_weight * math.sqrt(math.log(self.visits) / child.visits)\n",
        "                score = exploitation + exploration\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_children = [child]\n",
        "            elif score == best_score:\n",
        "                best_children.append(child)\n",
        "        return random.choice(best_children)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Node(move={self.move}, visits={self.visits}, reward={self.total_reward})\"\n"
      ],
      "metadata": {
        "id": "vtyzf3ZQ1Twr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Component 2: State Copying**\n",
        "\n",
        "Since MCTS simulates future moves many times, it’s important that changing the board in one simulation does not affect the real game.\n",
        "\n",
        "The second important component here is a function that makes copyies of the Ultimate-Tic-Tac-Toe Board\n",
        "\n",
        "Think about how to create an independent copy of:\n",
        "\n",
        "- The board's current layout,\n",
        "\n",
        "- Which player’s turn it is,\n",
        "\n",
        "- Any extra game metadata (e.g., where the next move must be played)."
      ],
      "metadata": {
        "id": "ZFAj2uAfztyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_ultimate_board(original):\n",
        "    new_board = UltimateBoard()\n",
        "    new_board.current_player = original.current_player\n",
        "    new_board.next_board = original.next_board\n",
        "    new_board.global_winner = original.global_winner\n",
        "\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            orig_sb = original.boards[i][j]\n",
        "            new_sb = SmallBoard()\n",
        "            new_sb.grid = orig_sb.grid.copy()\n",
        "            new_sb.winner = orig_sb.winner\n",
        "            new_sb.active = orig_sb.active\n",
        "            new_board.boards[i][j] = new_sb\n",
        "    return new_board\n"
      ],
      "metadata": {
        "id": "RJqbvep114qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Component 3: MCTS Main Loop**\n",
        "\n",
        "The Final component is the Main Loop/function itself. This is where we tie together all the components and complete the expansion and selection side of things.\n",
        "\n",
        "##This includes:\n",
        "\n",
        "Expanding New Moves\n",
        "- When reaching a node where there are still moves that haven't been tried:\n",
        "  - Pick one of the untried moves,\n",
        "  - Create a new node representing the resulting game state after playing that move,\n",
        "  - Add it to the tree.\n",
        "\n",
        "\n",
        "Simulating a Random Game (Rollout)\n",
        "- From the newly created node:\n",
        "  - Play random moves until the game ends (win, lose, or draw).\n",
        "  - This simulation does not have to be smart — just choose valid random moves at each step.\n",
        "\n",
        "Backpropagating Results\n",
        "- After the simulation finishes:\n",
        "  - Update statistics for all nodes along the path you traversed (from root to leaf).\n",
        "  - Each node should record:\n",
        "    - One more visit\n",
        "    - An updated total reward depending on the result.\n",
        "\n",
        "- When updating rewards:\n",
        "  - Reward should be viewed from the perspective of the root player,  not from the perspective of the node’s current player.\n",
        "\n",
        "Choosing the Final Move\n",
        "- After running many simulations:\n",
        "  - Look at all the children of the root node,\n",
        "  - Choose the child that was visited the most during search,\n",
        "  - Return the move that led to that child.\n"
      ],
      "metadata": {
        "id": "ma2mwkTSzxkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mcts(ultimate_board, iterations=500):\n",
        "    root = MCTSNode(copy_ultimate_board(ultimate_board))\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        node = root\n",
        "        path = [node]\n",
        "\n",
        "        # Selection\n",
        "        while not node.game_state.global_winner and node.is_fully_expanded() and node.children:\n",
        "          node = node.select_child()\n",
        "          path.append(node)\n",
        "\n",
        "        # If game ended in this node, results\n",
        "        if node.game_state.global_winner != 0:\n",
        "            reward = 0.0\n",
        "            if node.game_state.global_winner == root.game_state.current_player:\n",
        "                reward = 1.0\n",
        "            elif node.game_state.global_winner == -1:\n",
        "                reward = 0.5\n",
        "        else:\n",
        "            # Expansion\n",
        "            if node.untried_moves:\n",
        "                move = node.untried_moves.pop()\n",
        "                new_game = copy_ultimate_board(node.game_state)\n",
        "                new_game.play_move(*move)\n",
        "                child = MCTSNode(new_game, parent=node, move=move)\n",
        "                node.children.append(child)\n",
        "                path.append(child)\n",
        "                node = child\n",
        "\n",
        "            # Simulation\n",
        "            sim_game = copy_ultimate_board(node.game_state)\n",
        "            while sim_game.global_winner == 0 and sim_game.get_legal_moves():\n",
        "                move = random.choice(sim_game.get_legal_moves())\n",
        "                sim_game.play_move(*move)\n",
        "\n",
        "            # Determine reward from root player's perspective\n",
        "            if sim_game.global_winner == root.game_state.current_player:\n",
        "                reward = 1.0\n",
        "            elif sim_game.global_winner == -1:\n",
        "                reward = 0.5\n",
        "            else:\n",
        "                reward = 0.0\n",
        "\n",
        "        # Backpropagation\n",
        "        for n in path:\n",
        "            n.visits += 1\n",
        "            n.total_reward += reward\n",
        "\n",
        "    if not root.children:\n",
        "        return random.choice(ultimate_board.get_legal_moves())\n",
        "\n",
        "    best_move = max(root.children, key=lambda c: c.visits).move\n",
        "    return best_move"
      ],
      "metadata": {
        "id": "coijA93c18Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Testing Your MCTS Bot** (X Points)\n",
        "\n",
        "Have your MCTS Bot play against both the random agent for 5 games, and have it play against itself for 5 games.\n",
        "\n",
        "Modify its iteration count for each of the games, and then do the following for each game:\n",
        "\n",
        "- Print out a table containing the Iteration count and its corresponding winrate\n",
        "\n",
        "- Perform small write up on the results, and if they matched your predictions\n"
      ],
      "metadata": {
        "id": "EWYy9qcvgMO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "iteration_counts = [50,100,200,400,800]\n",
        "\n",
        "results_vs_random = []\n",
        "results_vs_self = []\n",
        "\n",
        "def mcts_agent_factory(iterations):\n",
        "    def agent(game):\n",
        "        return mcts(game, iterations=iterations)\n",
        "    return agent\n",
        "\n",
        "print(\"Testing MCTS vs Random Agent...\\n\")\n",
        "for iters in iteration_counts:\n",
        "    wins = 0\n",
        "    for _ in range(5):\n",
        "        mcts_agent = mcts_agent_factory(iters)\n",
        "        winner = play_botvbot_game(mcts_agent, random_bot)\n",
        "        if winner == 1:\n",
        "            wins += 1\n",
        "    winrate = wins / 5\n",
        "    results_vs_random.append((iters, winrate))\n",
        "    print(f\"Iterations: {iters} | Winrate vs Random: {winrate:.2f}\")\n",
        "\n",
        "print(\"\\nTesting MCTS vs MCTS...\\n\")\n",
        "for iters in iteration_counts:\n",
        "    wins = 0\n",
        "    for _ in range(5):\n",
        "        mcts_agent = mcts_agent_factory(iters)\n",
        "        winner = play_botvbot_game(mcts_agent, mcts_agent_factory(iters))\n",
        "        if winner == 1:\n",
        "            wins += 1\n",
        "    winrate = wins / 5\n",
        "    results_vs_self.append((iters, winrate))\n",
        "    print(f\"Iterations: {iters} | Winrate vs Self (X winrate): {winrate:.2f}\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_random = pd.DataFrame(results_vs_random, columns=[\"Iterations\", \"Winrate_vs_Random\"])\n",
        "df_self = pd.DataFrame(results_vs_self, columns=[\"Iterations\", \"Winrate_vs_SelfPlay\"])\n",
        "\n",
        "print(\"\\nSummary Table: MCTS vs Random\")\n",
        "print(df_random)\n",
        "\n",
        "print(\"\\nSummary Table: MCTS vs MCTS\")\n",
        "print(df_self)"
      ],
      "metadata": {
        "id": "PicxfjC6G79R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: Improving the Bot** (X Points)\n",
        "\n",
        "Now that we have a basic implementation of MCTS, Lets improve it.\n",
        "\n",
        "Lets look at Improving the rollout/simulation function and the Reward System for our bot.\n",
        "\n",
        "Write an Enhanced Version of your MCTS Bot that implements the following to your Rollout Function and Reward System.\n",
        "\n",
        "- A Smarter Rollout/Simulation Function (Non-random simulation heuristics)\n",
        "\n",
        "- Develop better rewards:\n",
        "\n",
        "- Improving the Node Selection Formula:\n",
        "\n",
        "If you would like some help or additional reading to understand this better:\n",
        "\n",
        "- [Reading 1 (Reccomended)](https://www.uttt.ai/blog)\n",
        "- [Reading 2 (Complicated)](https://suragnair.github.io/posts/alphazero.html)\n",
        "- [Reading 3 (LONG)](https://philhchen.github.io/files/tictactoe-paper.pdf)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ErlsBZenfAny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Improvement 1: Nodes!**\n",
        "\n",
        "The first of these Improvements is to the MCTS Node.\n",
        "\n",
        "In the basic MCTS version, all legal moves were treated equally during expansion and selection. But not all moves are equal.\n",
        "\n",
        "Some positions (like the center of a board) are much stronger than others — and experienced players (and bots!) often prioritize these.\n",
        "\n",
        "You should modify how your nodes prioritize which moves to try. Specifically:\n",
        "\n",
        "- Assign a “prior score” or weight to each move, based on how good it seems before simulating it.\n",
        "\n",
        "- Use this prior to bias the move selection during tree search.\n",
        "\n",
        "Here are some simple heuristics you can use to score moves:\n",
        "\n",
        "- Moves to the center of a small board are better.\n",
        "\n",
        "- Corner positions are better than sides.\n",
        "\n",
        "- If a move wins a small board, it's much better.\n",
        "\n",
        "- Moves that send the opponent to a finished board (giving you more freedom) are better.\n",
        "\n",
        "Another Improvement is the formula used!\n",
        "\n",
        "- If you used UCB1, you should improve the formula to the PUCT\n",
        "\n",
        "(Read [this](https://towardsdatascience.com/playing-chess-with-a-generalized-ai-b83d64ac71fe/) to get some insight)"
      ],
      "metadata": {
        "id": "JNm3NRCNKRTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class EnhancedMCTSNode:\n",
        "    def __init__(self, game_state, parent=None, move=None):\n",
        "        self.game_state = game_state\n",
        "        self.parent = parent\n",
        "        self.move = move\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.total_reward = 0.0\n",
        "        self.prior = 1.0  # Prior probability for PUCT\n",
        "        self.untried_moves = self._prioritized_moves(game_state.get_legal_moves())\n",
        "\n",
        "    def _prioritized_moves(self, moves):\n",
        "        prioritized = []\n",
        "        for move in moves:\n",
        "            small_r, small_c = move[2], move[3]\n",
        "            priority = 1.0\n",
        "            if (small_r, small_c) in [(1,1)]:  # Center\n",
        "                priority = 1.5\n",
        "            elif (small_r%2 == 0) and (small_c%2 == 0):  # Corners\n",
        "                priority = 1.2\n",
        "            prioritized.append((move, priority))\n",
        "        return prioritized\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        return len(self.untried_moves) == 0\n",
        "\n",
        "    def select_child(self, c_puct=1.5):\n",
        "        # PUCT formula: argmax(Q(s,a) + c_puct * P(s,a) * sqrt(N(s)) / (1 + N(s,a)))\n",
        "        total_n = math.sqrt(self.visits)\n",
        "        best_score = -float('inf')\n",
        "        best_child = None\n",
        "\n",
        "        for child in self.children:\n",
        "            if child.visits == 0:\n",
        "                q_value = 0\n",
        "            else:\n",
        "                q_value = child.total_reward / child.visits\n",
        "\n",
        "            puct_value = c_puct * child.prior * total_n / (1 + child.visits)\n",
        "            score = q_value + puct_value\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_child = child\n",
        "        return best_child\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Node(move={self.move}, visits={self.visits}, reward={self.total_reward})\"\n"
      ],
      "metadata": {
        "id": "zCPES9KSKR1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Improvement 2: Reward Systems!**\n",
        "\n",
        "Your basic agent treated outcomes as either a win (1), tie (0.5), or loss (0).\n",
        "This is very coarse and only tells the agent if it won at the end — it doesn’t help during the rest of the game.\n",
        "\n",
        "Instead, you should define a reward function that can give credit for partial progress — even if the game hasn’t ended yet.\n",
        "\n",
        "Improve/Create the system so that it rewards:\n",
        "\n",
        "- Rewards winning a small board (e.g., +0.1),\n",
        "\n",
        "- Penalizes losing a small board (e.g., -0.1),\n",
        "\n",
        "- Gives extra reward for controlling multiple boards in a row, or the center board,\n",
        "\n",
        "- Still gives a larger reward for winning the full game (e.g., +1.0), and a smaller reward for drawing (e.g., 0.4),\n",
        "\n",
        "- Optionally uses a squashing function (like tanh) to keep values in a reasonable range.\n",
        "\n",
        "This lets your MCTS agent backpropagate better signals — not just \"Did I win?\" but rather \"How good is this board overall?\""
      ],
      "metadata": {
        "id": "3wEEx69pLa1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_reward(final_state, root_player):\n",
        "    if final_state.global_winner == root_player:\n",
        "        return 1.0\n",
        "    if final_state.global_winner == -1:\n",
        "        return 0.4\n",
        "    if final_state.global_winner == 3 - root_player:\n",
        "        return -1.0\n",
        "\n",
        "    score = 0\n",
        "    small_board_counts = np.zeros((3,3))\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            if final_state.boards[i][j].winner == root_player:\n",
        "                score += 0.1\n",
        "                small_board_counts[i,j] = 1\n",
        "            elif final_state.boards[i][j].winner == 3 - root_player:\n",
        "                score -= 0.1\n",
        "                small_board_counts[i,j] = -1\n",
        "\n",
        "    for i in range(3):\n",
        "        row_sum = np.sum(small_board_counts[i,:])\n",
        "        col_sum = np.sum(small_board_counts[:,i])\n",
        "        if abs(row_sum) == 3 or abs(col_sum) == 3:\n",
        "            score += 0.3 * np.sign(row_sum if abs(row_sum)==3 else col_sum)\n",
        "\n",
        "    return np.tanh(score)"
      ],
      "metadata": {
        "id": "EvNCmfGOLbPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Improvement 3: Main Loop (Propogation/Simulation)**\n",
        "\n",
        "In your original MCTS agent, simulations were completely random.\n",
        "This works okay for simple games, but in Ultimate Tic-Tac-Toe, random play often leads to noisy, misleading results.\n",
        "\n",
        "Your goal is to make the simulation (or \"rollout\") smarter, so it gives better estimates of move quality.\n",
        "\n",
        "Try to update your simulation logic so that it uses a heuristic.\n",
        "\n",
        "Ex.\n",
        " - Prefer moves that complete small boards.\n",
        " - Prioritize moves that block the opponent from winning small boards.\n",
        " - Avoid bad or useless moves (e.g., playing moves that results in letting the opponent play in any board).\n",
        "\n",
        "You don’t need to build a full search or evaluation algorithm inside the simulation. But even a few simple heuristics can make the playouts way more realistic."
      ],
      "metadata": {
        "id": "0q37kbUAVvso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enhanced_simulation(game_state):\n",
        "    sim_game = copy_ultimate_board(game_state)\n",
        "\n",
        "    while sim_game.global_winner == 0 and sim_game.get_legal_moves():\n",
        "        moves = sim_game.get_legal_moves()\n",
        "        best_moves = []\n",
        "\n",
        "        for move in moves:\n",
        "            temp_game = copy_ultimate_board(sim_game)\n",
        "            temp_game.play_move(*move)\n",
        "\n",
        "            big_r, big_c = move[0], move[1]\n",
        "            if temp_game.boards[big_r][big_c].winner == sim_game.current_player:\n",
        "                best_moves.append(move)\n",
        "\n",
        "        if best_moves:\n",
        "            move = random.choice(best_moves)\n",
        "        else:\n",
        "            # Positional fallback: center > corner > edge\n",
        "            scored = []\n",
        "            for move in moves:\n",
        "                sr, sc = move[2], move[3]\n",
        "                if (sr, sc) == (1, 1):\n",
        "                    score = 3\n",
        "                elif sr % 2 == 0 and sc % 2 == 0:\n",
        "                    score = 2\n",
        "                else:\n",
        "                    score = 1\n",
        "                scored.append((move, score))\n",
        "\n",
        "            max_score = max(score for _, score in scored)\n",
        "            fallback = [m for m, s in scored if s == max_score]\n",
        "            move = random.choice(fallback)\n",
        "\n",
        "        sim_game.play_move(*move)\n",
        "\n",
        "    return sim_game\n",
        "\n",
        "\n",
        "def enhanced_mcts(ultimate_board, iterations=800):\n",
        "    root = EnhancedMCTSNode(copy_ultimate_board(ultimate_board))\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        node = root\n",
        "        path = [node]\n",
        "\n",
        "        # Selection\n",
        "        while not node.game_state.global_winner and node.is_fully_expanded() and node.children:\n",
        "            node = node.select_child()\n",
        "            path.append(node)\n",
        "\n",
        "        # Expansion\n",
        "        if node.untried_moves and not node.game_state.global_winner:\n",
        "            move, prior = node.untried_moves.pop()\n",
        "            new_game = copy_ultimate_board(node.game_state)\n",
        "            new_game.play_move(*move)\n",
        "            child = EnhancedMCTSNode(new_game, parent=node, move=move)\n",
        "            child.prior = prior\n",
        "            node.children.append(child)\n",
        "            path.append(child)\n",
        "            node = child\n",
        "\n",
        "        # Simulation\n",
        "        final_state = enhanced_simulation(node.game_state)\n",
        "\n",
        "        # Reward calculation\n",
        "        reward = calculate_reward(final_state, root.game_state.current_player)\n",
        "\n",
        "        # Backpropagation\n",
        "        for n in path:\n",
        "            n.visits += 1\n",
        "            n.total_reward += reward\n",
        "\n",
        "    if not root.children:\n",
        "        return random.choice(ultimate_board.get_legal_moves())\n",
        "\n",
        "    best_child = max(root.children, key=lambda c: c.visits)\n",
        "    return best_child.move\n"
      ],
      "metadata": {
        "id": "6I_UmCdEfs7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 4: The Gauntlet** (X Points)\n",
        "\n",
        "Lets do a set of 1v1 tests between your MCTS Agent and the less than perfect ones below.\n",
        "\n",
        "The following are 3 additional Agents, each with a special quirk.\n",
        "\n",
        "###For each agent:\n",
        "- Complete the Predictions\n",
        "\n",
        "- Have your Monte-Carlos Agent Play against the challenge Agent, at 4 different iteration counts. (5 game for each is good enough)\n",
        "\n",
        "- Print out a table containing the Iteration count and its corresponding winrate\n",
        "\n",
        "- Use the Visualizer to create a final board summary.\n",
        "\n",
        "- Perform small write up on the results, and if they matched your predictions\n",
        "\n",
        "Hint: You may be able to reuse your previous test code for Part 2's Testing here!"
      ],
      "metadata": {
        "id": "pwRT5EgUfBHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Round 1: Improved vs Sir Abserd\n",
        "\n",
        "Sir Abserd is as random as random comes, and makes moves with no rhyme or reason.\n",
        "\n",
        "Abserd:\n",
        " - Makes Random Moves"
      ],
      "metadata": {
        "id": "M3J5nCoOlnvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_ser_abserd(board):\n",
        "    return random.choice(board.get_legal_moves())"
      ],
      "metadata": {
        "id": "bkDy3Xgxfqpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction:\n",
        "- Give a Range, how many moves do you think it'll take to win?\n",
        "- At what Iteration Count do you think it'll take your bot to reach above 50% win rate."
      ],
      "metadata": {
        "id": "45fPNexmBb_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "iteration_counts = [50,100,200,400,800]\n",
        "\n",
        "results = []\n",
        "\n",
        "def mcts_agent_factory(iterations):\n",
        "    def agent(game):\n",
        "        return enhanced_mcts(game, iterations=iterations)\n",
        "    return agent\n",
        "\n",
        "print(\"Testing MCTS vs Abserd\\n\")\n",
        "for iters in iteration_counts:\n",
        "    wins = 0\n",
        "    for _ in range(5):\n",
        "        mcts_agent = mcts_agent_factory(iters)\n",
        "        winner = play_botvbot_game(mcts_agent, agent_ser_abserd)\n",
        "        if winner == 1:\n",
        "            wins += 1\n",
        "    winrate = wins / 5\n",
        "    results.append((iters, winrate))\n",
        "    print(f\"Iterations: {iters} | Winrate vs Abserd: {winrate:.2f}\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_random = pd.DataFrame(results, columns=[\"Iterations\", \"Winrate_vs_Abserd\"])\n",
        "\n",
        "print(\"\\nSummary Table: MCTS vs Abserd\")\n",
        "print(df_random)"
      ],
      "metadata": {
        "id": "Y0GChOZeBbgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Round 2: Monte-Carlo vs Near Seer**\n",
        "Near Seer can see the future, so long as it's within a 3x3 block.\n",
        "\n",
        "Near Seer:\n",
        "  - Tries to make a win on a small board if it sees one"
      ],
      "metadata": {
        "id": "cP0wYRfhlqNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_near_seer(board):\n",
        "    moves = board.get_legal_moves()\n",
        "    for move in moves:\n",
        "        big_r, big_c, small_r, small_c = move\n",
        "        small_board = board.boards[big_r][big_c]\n",
        "        temp_grid = small_board.grid.copy()\n",
        "        temp_grid[small_r, small_c] = board.current_player\n",
        "\n",
        "        # Check for immediate win in small board\n",
        "        for i in range(3):\n",
        "            if all(temp_grid[i, :] == board.current_player) or all(temp_grid[:, i] == board.current_player):\n",
        "                return move\n",
        "        if all(np.diag(temp_grid) == board.current_player) or all(np.diag(np.fliplr(temp_grid)) == board.current_player):\n",
        "            return move\n",
        "    return random.choice(moves)"
      ],
      "metadata": {
        "id": "aD1dUUl8lnY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction:\n",
        "- Give a Range, how many moves do you think it'll take to win?\n",
        "- At what Iteration Count do you think it'll take your bot to reach above 50% win rate."
      ],
      "metadata": {
        "id": "QyNBQu2EXrvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "iteration_counts = [50,100,200,400,800]\n",
        "\n",
        "results = []\n",
        "\n",
        "def mcts_agent_factory(iterations):\n",
        "    def agent(game):\n",
        "        return enhanced_mcts(game, iterations=iterations)\n",
        "    return agent\n",
        "\n",
        "print(\"Testing MCTS vs Seer\\n\")\n",
        "for iters in iteration_counts:\n",
        "    wins = 0\n",
        "    for _ in range(5):\n",
        "        mcts_agent = mcts_agent_factory(iters)\n",
        "        winner = play_botvbot_game(mcts_agent, agent_near_seer)\n",
        "        if winner == 1:\n",
        "            wins += 1\n",
        "    winrate = wins / 5\n",
        "    results.append((iters, winrate))\n",
        "    print(f\"Iterations: {iters} | Winrate vs Seer: {winrate:.2f}\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_random = pd.DataFrame(results, columns=[\"Iterations\", \"Winrate_vs_Seer\"])\n",
        "\n",
        "print(\"\\nSummary Table: MCTS vs Seer\")\n",
        "print(df_random)"
      ],
      "metadata": {
        "id": "vuXG7XFdXq_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Round 3: Monte-Carlo vs Count Erfit**\n",
        "\n",
        "Count Erfit tries to mirror his last move, and if he can't, he'll mirror your last move.\n",
        "\n",
        "Count Erfit:\n",
        "  - First, tries to play same (small_r, small_c) as last own move\n",
        "  - If not available, tries to play the exact opponent last move\n",
        "  - If not, plays random\n"
      ],
      "metadata": {
        "id": "ph1nDUsPlp-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_erfit(board, last_self_move, last_opponent_move):\n",
        "    moves = board.get_legal_moves()\n",
        "\n",
        "    if last_self_move:\n",
        "        _, _, last_small_r, last_small_c = last_self_move\n",
        "        for move in moves:\n",
        "            if move[2] == last_small_r and move[3] == last_small_c:\n",
        "                return move\n",
        "\n",
        "    if last_opponent_move:\n",
        "        target_move = last_opponent_move\n",
        "        if target_move in moves:\n",
        "            return target_move\n",
        "\n",
        "    # Otherwise random\n",
        "    return random.choice(moves)\n",
        "#Wrapper\n",
        "def agent_count_erfit():\n",
        "    last_self_move = [None]\n",
        "    last_opponent_move = [None]\n",
        "\n",
        "    def agent(game):\n",
        "        move = count_erfit(game, last_self_move[0], last_opponent_move[0])\n",
        "        last_opponent_move[0] = move\n",
        "        last_self_move[0] = move\n",
        "        return move\n",
        "\n",
        "    return agent"
      ],
      "metadata": {
        "id": "4Rf25SUSlnMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction:\n",
        "- Give a Range, how many moves do you think it'll take to win?\n",
        "- At what Iteration Count do you think it'll take your bot to reach above 50% win rate."
      ],
      "metadata": {
        "id": "DKf6I722FDnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "iteration_counts = [50,100,200,400,800]\n",
        "\n",
        "results = []\n",
        "\n",
        "def mcts_agent_factory(iterations):\n",
        "    def agent(game):\n",
        "        return enhanced_mcts(game, iterations=iterations)\n",
        "    return agent\n",
        "\n",
        "print(\"Testing MCTS vs Count\\n\")\n",
        "for iters in iteration_counts:\n",
        "    wins = 0\n",
        "    for _ in range(5):\n",
        "        mcts_agent = mcts_agent_factory(iters)\n",
        "        winner = play_botvbot_game(mcts_agent, agent_count_erfit())\n",
        "        if winner == 1:\n",
        "            wins += 1\n",
        "    winrate = wins / 5\n",
        "    results.append((iters, winrate))\n",
        "    print(f\"Iterations: {iters} | Winrate vs Count: {winrate:.2f}\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_random = pd.DataFrame(results, columns=[\"Iterations\", \"Winrate_vs_Count\"])\n",
        "\n",
        "print(\"\\nSummary Table: MCTS vs Count\")\n",
        "print(df_random)"
      ],
      "metadata": {
        "id": "gf49UqHnFFG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Final Test: Improved Monte-Carlo Vs Unimproved Monte-Carlo**\n",
        "\n",
        "At the end of the Gaunlet, the greatest battle is against one-self.\n",
        "\n",
        "For this test, have the your Improved Monte Carlo Implementation compete vs the basic version from Part 2.\n",
        "\n",
        "This agent should match the improved version's Iteration Count\n"
      ],
      "metadata": {
        "id": "gt17PkN_ECf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction:\n",
        "- Give a Range, how many moves do you think it'll take to win?\n",
        "- At what Iteration Count do you think it'll take your bot to reach above 50% win rate."
      ],
      "metadata": {
        "id": "v9_pAquaFGMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "iteration_counts = [50,100,200,400,800]\n",
        "\n",
        "results = []\n",
        "\n",
        "def mcts_agent_factory(iterations):\n",
        "    def agent(game):\n",
        "        return enhanced_mcts(game, iterations=iterations)\n",
        "    return agent\n",
        "\n",
        "def simple_agent_factory(iterations):\n",
        "    def agent(game):\n",
        "        return mcts(game, iterations=iterations)\n",
        "    return agent\n",
        "\n",
        "print(\"Testing MCTS vs Basic\\n\")\n",
        "for iters in iteration_counts:\n",
        "    wins = 0\n",
        "    for _ in range(5):\n",
        "        mcts_agent = mcts_agent_factory(iters)\n",
        "        simple_agent = simple_agent_factory(iters)\n",
        "        winner = play_botvbot_game(mcts_agent, simple_agent)\n",
        "        if winner == 1:\n",
        "            wins += 1\n",
        "    winrate = wins / 5\n",
        "    results.append((iters, winrate))\n",
        "    print(f\"Iterations: {iters} | Winrate vs Baic: {winrate:.2f}\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_random = pd.DataFrame(results, columns=[\"Iterations\", \"Winrate_vs_Basic\"])\n",
        "\n",
        "print(\"\\nSummary Table: Enhanced vs Basic\")\n",
        "print(df_random)"
      ],
      "metadata": {
        "id": "uGBN1KP5EDEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Discussion!\n",
        "\n",
        "Think about the 4 tests we have done with the Improved Monte-Carlo Implementation.\n",
        "\n",
        "Can the Monte-Carlo bot be improved any further? If so, how? If not, why not?"
      ],
      "metadata": {
        "id": "GpyqNQ0HluJf"
      }
    }
  ]
}